{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b3f0e1-2b4f-4296-a4d1-e6da038c0a7c",
   "metadata": {},
   "source": [
    "# Basic Image Query\n",
    "Create a simple function to send an image to a vision model and get a response to a general question about the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2cd1f8-855d-484a-ba61-0bc61d203ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(encoded_image, user_query, assistant_prompt=\"You are a helpful assistant. Answer the following user query in 1 or 2 sentences: \"):\n",
    "    \"\"\" Send image and query to the model and get a response.\"\"\"\n",
    "\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "    {\n",
    "    \"type\":\"text\",\n",
    "    \"text\":assistant_prompt + user_query\n",
    "    },\n",
    "    {\n",
    "    \"type\": \"image_url\",\n",
    "    \"image_url\": {\n",
    "    \"url\": \"data/image.......\" + encoded_image,\n",
    "    }}]}]\n",
    "    response = model.chat(messages=messages)\n",
    "    return repsonse['choices'][0]['message']['content']\n",
    "\n",
    "// Example usage:\n",
    "user_query = \"Describe the photo\"\n",
    "response = generate_model_response(encoded_image[0], user_query)\n",
    "print(\"Description: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25067f-9c97-4087-9ff3-e6798bdc9a3a",
   "metadata": {},
   "source": [
    "# Basic Object Detection\n",
    "Use the vision model to detect and count objects in images by asking specific questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90a2d56-6799-45bf-af04-202f0b330706",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Detection examples for various use cases\n",
    "image = encoded_image[i]\n",
    "\n",
    "result = generate_model_response(\n",
    "    image, \n",
    "    \"How many cars are there in this image?\"\n",
    ")\n",
    "print(\"Cars detected: \", result)\n",
    "\n",
    "// Examine results \n",
    "result = generate_model_response(\n",
    "    image,\n",
    "    \"What color is the woman's jacket in this image?\"\n",
    ")\n",
    "print(\"Clothing analysis: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44481afc-77b9-4b23-912d-38a7fd41127b",
   "metadata": {},
   "source": [
    "# Creating message for the vision model\n",
    "Format a request with both text and image data to send to the multimodal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848d86b-526d-4f2c-b34f-a369e8337d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vision_image(prompt, encoded_image):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt \n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"data/image......\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4121930-ae05-4773-8190-fd48b5c4d073",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "Create and activate a virtual environment, then install necessary packages for multimodal applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c27da2-f21a-4780-911b-deb546027af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "python3.11 -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install ibm-watsonx-ai==1.1.20 image==1.5.33 flask requests==2.32.0\n",
    "pip install torch torchvision scikit-learn pillow gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c0aab-3ed1-4d3a-a350-1f88c960d43b",
   "metadata": {},
   "source": [
    "# Flask integration for vision AI and web app\n",
    "Basic Flask setup to create a web application with vision AI capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a95953-edaa-4e00-9cd8-802577bde4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request\n",
    "app = Flask(__name__)\n",
    "@app.route(\"/\", methods=[\"GET\",\"POST\"])\n",
    "def index():\n",
    "    if request.method == \"POST\":\n",
    "        # Retrieve user prompts\n",
    "        user_query = requests.form.get(\"user_query\")\n",
    "        uploaded_file = requests.files.get(\"file\")\n",
    "        if uploaded_file:\n",
    "            # Process the uploaded image\n",
    "            encoded_image = input_image_setup(uploaded_file)\n",
    "            # Generate the model's response\n",
    "            response = generate_model_response(encoded_image, user_query, assistant_prompt)\n",
    "            # Render the result\n",
    "            return render_template(\"index.html\", user_query=user_query, response=response)\n",
    "        return render_template(\"index.html\")\n",
    "\n",
    "if name == \"main\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890498b3-ef3e-4474-8ac9-0ee4075ff2cb",
   "metadata": {},
   "source": [
    "# Image encoding from URLs\n",
    "Load and encode multiple images from URLs to base64 format for batch processing with vision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010188da-7342-45c0-ab9e-1eca24c2b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "// Define image URLs\n",
    "url_image_1 = ‘https://example.com/image1.jpg'\n",
    "url_image_2 = ‘https://example.com/image2.jpg'\n",
    "image_urls = [url_image_1, url_image_2]\n",
    "\n",
    "// Encode all images\n",
    "encoded_images = []\n",
    "for url in image_urls:\n",
    "    encoded_images.append(\n",
    "        base64.b64encode(\n",
    "            requests.get(url).content\n",
    "        ).decode(\"utf-8\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17e7758-4e32-4941-a63e-6fa925788f2c",
   "metadata": {},
   "source": [
    "# Image encoding from uploads\n",
    "Convert and encoded image and file it to base64 format for batch processing with vision models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cb445-317e-495b-91a5-bbc5a29ca562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image \n",
    "from io import Bytes10\n",
    "\n",
    "def input_image_setup(uploaded_file):\n",
    "    if uploaded_file is not None:\n",
    "        # Read file into bytes\n",
    "        bytes_data = uploaded_file.read()\n",
    "        # Encode image to base64 string\n",
    "        encoded_image = base64.b64encode(bytes_data).decode(\"utf-8\")\n",
    "        return encoded_image\n",
    "    else:\n",
    "        return FileNotFoundError(\"No file uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2731e70d-ae19-417a-b3fa-d348cbe2f170",
   "metadata": {},
   "source": [
    "# Similarity Match\n",
    "Find the closest matching image in a dataset based on cosine similarity of vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0ed88-15d7-43f1-980e-3c41b0b7bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_closest_match(user_vector, dataset):\n",
    "    \"\"\" Find closest match based on cosine similarity \"\"\"\n",
    "    try:\n",
    "        # Stack all vectors from dataset\n",
    "        dataset_vectors = np.vstack(dataset[\"Embedding\"].dropna().values)\n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(user_vector.reshape(-1,1), dataset_vectors)\n",
    "        # Find highest similarity index\n",
    "        closest_index = np.argmax(similarities)\n",
    "        similarity_score = similarities[0][closest_index]\n",
    "        # Get corresponding dataset row\n",
    "        closest_row = dataset.iloc[closest_index]\n",
    "        return closest_row, similariy_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding closest match {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f51909-80e3-4553-a117-8dce67d03b2d",
   "metadata": {},
   "source": [
    "# Vector Embeddings for images\n",
    "Convert images to vector embeddings for similarity matching using a pre-trained ResNet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2e404-2649-4c97-ab7e-9fa62b127ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as resnet50\n",
    "import numpy as np\n",
    "\n",
    "class ImageProcessor:\n",
    "    def init(self, image_size=(224, 224),\n",
    "             norm_mean=(0.485, 0.456, 0.406),\n",
    "             norm_std=[0.229, 0.224, 0.225]):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = resnet50(pretrained=True).to(self.device)\n",
    "        self.model.eval() # Set model to evaluation mode\n",
    "        # image preprocessing pipeline\n",
    "        self.preprocess = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=norm_mean, std=norm_std),\n",
    "        ])\n",
    "\n",
    "    def encode_image(self, image_input, is_url=True):\n",
    "        try:\n",
    "            if is_url:\n",
    "                # Fetch image from URL\n",
    "                response = requests.get(image_input)\n",
    "                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            else:\n",
    "                # Load from local file\n",
    "                image = Image.open(image_input).convert(\"RGB\")\n",
    "            # Convert image to base64\n",
    "            buffered = BytesIO()\n",
    "            image.save(buffered, format=\"JPEG\")\n",
    "            base64_string = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "            # Get feature vector using ResNet50\n",
    "            input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                features = self.model(input_tensor)\n",
    "            # Convert to numpy array\n",
    "            feature_vector = features.cpu().numpy().flatten()\n",
    "            return {\"base64\": base64_string, \"vector\": feature_vector}\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding image: {e}\")\n",
    "            return {\"base64\": None, \"vector\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3da28-89a3-49e2-8b2c-7491da95094f",
   "metadata": {},
   "source": [
    "# Vision Model Initialization\n",
    "Set up credentials and initialize the Llama 3.2 Vision Instruct model through Watsonx AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33d538-fecc-4256-8311-a040941fd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextChatParameters\n",
    "\n",
    "credentials = Credentials(\n",
    "    url = \"....\",\n",
    "    api_key = \"YOUR_API_KEY\",\n",
    ")\n",
    "client = APIClient(crendentials)\n",
    "\n",
    "model_id = \"meta-llama/llama-3-2-90b-vision-instruct\"\n",
    "project_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a54278-1de1-43aa-b24f-cd2893be77d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51225ab6-9a3d-4bfd-9a5f-d2404d0ab431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57c668-ff44-4991-a326-8dd242608b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da911c54-b2f1-4a54-bc89-40e06518853f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578cf17-8bc0-46cc-9095-3587d5ecbfd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39272f85-f780-4657-bec6-95aaf3053969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676128f6-4a40-4ad3-a155-aeb47eb6d44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055ca7f-2bcc-422f-93fd-862e5571031d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2b6ce-062c-49ac-942e-d77cbe50b4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41791e26-7387-4343-9c1c-7675594be91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
