{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Build a Tool Calling Agent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **1** hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll explore the powerful capabilities of tool calling in large language models (LLMs) to build advanced AI agents that can interact with external systems. You'll learn how to create custom tools that enable an LLM to perform specific actions, from extracting video IDs to fetching YouTube transcripts and metadata. Through hands-on examples, you'll first implement manual tool calling to understand the underlying mechanics, then build a flexible YouTube interaction system that can search videos, extract transcripts, fetch trending content, and generate summaries. By the end of this lab, you'll understand how to construct both fixed-sequence and recursive tool-calling chains, allowing your AI assistants to dynamically decide which tools to use and when to use them, creating truly intelligent agents that can reason about and interact with the world around them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "   <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "   <li>\n",
    "       <a href=\"#Setup\">Setup</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "           <li><a href=\"#Importing-Required-Libraries\">Importing required libraries</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Tools\">Tools</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Defining-video-ID-extraction-tool\">Defining video ID extraction tool</a></li>\n",
    "           <li><a href=\"#Tool-list\">Tool list</a></li>\n",
    "           <li><a href=\"#Defining-transcript-fetching-tool\">Defining transcript fetching tool</a></li>\n",
    "           <li><a href=\"#Defining-YouTube-search-tool\">Defining YouTube search tool</a></li>\n",
    "           <li><a href=\"#Defining-metadata-extraction-tool\">Defining metadata extraction tool</a></li>\n",
    "           <li><a href=\"#Defining-trending-videos-tool\">Defining trending videos tool</a></li>\n",
    "           <li><a href=\"#Defining-thumbnail-retrieval-tool\">Defining thumbnail retrieval tool</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Binding-tools\">Binding tools</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#How-the-LLM-calls-a-tool\">How the LLM calls a tool</a></li>\n",
    "           <li><a href=\"#LangChain-tool-binding-process\">LangChain tool binding process</a></li>\n",
    "           <li><a href=\"#Extracting-tool-call-information\">Extracting tool call information</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Automating-the-tool-calling-process\">Automating the tool calling process</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Building-the-summarization-chain\">Building the summarization chain</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "   <li>\n",
    "       <a href=\"#Recursive-chain-flow\">Recursive chain flow</a>\n",
    "       <ol>\n",
    "           <li><a href=\"#Defining-the-core-processing-logic\">Defining the core processing logic</a></li>\n",
    "           <li><a href=\"#Building-the-complete-universal-chain\">Building the complete universal chain</a></li>\n",
    "       </ol>\n",
    "   </li>\n",
    "</ol>\n",
    "\n",
    "<li><a href=\"#Exercise\">Exercise</a></li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- Create custom tools that extend the capabilities of language models\n",
    "- Build both manual and automated tool calling chains\n",
    "- Implement recursive tool calling for dynamic, multi-step operations\n",
    "- Develop AI agents that can interact with YouTube's content programmatically\n",
    "- Apply tool calling techniques to extract, process, and summarize information from external sources\n",
    "- Design flexible workflows that allow LLMs to reason about when and how to use available tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`pytube`](https://pytube.io/en/latest/) for accessing YouTube videos and their metadata programmatically.\n",
    "*   [`youtube-transcript-api`](https://github.com/jdepoix/youtube-transcript-api) for fetching transcripts from YouTube videos.\n",
    "*   [`langchain`](https://python.langchain.com/docs/get_started/introduction) for building tool-enabled LLM applications.\n",
    "*   [`langchain-community`](https://python.langchain.com/docs/integrations/providers/) for additional LangChain integrations.\n",
    "*   [`langchain-openai`](https://python.langchain.com/docs/integrations/llms/openai) for connecting to OpenAI's language models.\n",
    "*   [`yt-dlp`](https://github.com/yt-dlp/yt-dlp) for enhanced YouTube data extraction capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pytube \n",
    "%pip install youtube-transcript-api==1.1.0\n",
    "%pip install langchain-community==0.3.16\n",
    "%pip install langchain==0.3.23\n",
    "%pip install langchain-openai==0.3.14\n",
    "%pip install yt-dlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pytube import YouTube\n",
    "from langchain_core.tools import tool\n",
    "from IPython.display import display, JSON\n",
    "import yt_dlp\n",
    "from typing import List, Dict\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages import ToolMessage\n",
    "import json\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress pytube errors\n",
    "import logging\n",
    "pytube_logger = logging.getLogger('pytube')\n",
    "pytube_logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress yt-dlp warnings\n",
    "yt_dpl_logger = logging.getLogger('yt_dlp')\n",
    "yt_dpl_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the language model that will power your tool calling capabilities. This code sets up a GPT-4o-mini model using the OpenAI provider through LangChain's interface, which you'll use to process queries and decide which tools to call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Disclaimer\n",
    "This lab uses LLMs provided by OpenAI. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to configure your own API keys. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API key. This lab uses the `init_chat_model` function from `langchain`. To use the model you must set the environment variable `OPENAI_API_KEY` to your OpenAI API key. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your OpenAI API key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating custom tools with LangChain\n",
    "\n",
    "### Anatomy of a tool\n",
    "\n",
    "Let's provide the basic building blooks a  tool, consider the following tools:\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def tool_name(input_param: input_type) -> output_type:\n",
    "   \"\"\"\n",
    "   Clear description of what the tool does.\n",
    "   \n",
    "   Args:\n",
    "       input_param (input_type): Description of this parameter\n",
    "   \n",
    "   Returns:\n",
    "       output_type: Description of what is returned\n",
    "   \"\"\"\n",
    "   # Function implementation\n",
    "   result = process(input_param)\n",
    "   return result\n",
    "```\n",
    "\n",
    "\n",
    "## Key components\n",
    "\n",
    "1. **@tool decorator**\n",
    "   - Registers the function with LangChain\n",
    "   - Creates tool attributes (.name, .description, .func)\n",
    "   - Generates JSON schema for validation\n",
    "   - Transforms regular functions into callable tools\n",
    "\n",
    "2. **Function name**\n",
    "   - Used by LLM to select appropriate tool\n",
    "   - Used as reference in chains and tool mappings\n",
    "   - Appears in tool call logs for debugging\n",
    "   - Should clearly indicate the tool's purpose\n",
    "\n",
    "3. **Type annotations**\n",
    "   - Enable automatic input validation\n",
    "   - Create schema for parameters\n",
    "   - Allow proper serialization of inputs/outputs\n",
    "   - Help LLM understand required input formats\n",
    "\n",
    "4. **Docstring**\n",
    "   - Provides context for the LLM to decide when to use the tool\n",
    "   - Documents parameter requirements\n",
    "   - Explains expected outputs and behavior\n",
    "   - Critical for tool selection by the LLM\n",
    "\n",
    "5. **Implementation**\n",
    "   - Executes the actual operation\n",
    "   - Handles errors appropriately\n",
    "   - Returns properly formatted results\n",
    "   - Should be efficient and robust\n",
    "\n",
    "\n",
    "### Defining video ID extraction tool\n",
    "\n",
    "Now you'll define a function `extract_video_id` by denoting it as a tool that will help you to extract the video ID from a given URL. This is necessary because many YouTube API operations, including transcript extraction, require the video ID rather than the complete URL. The function uses regular expressions to handle different YouTube URL formats (standard, shortened, and embedded) and extract the 11-character video ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def extract_video_id(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the 11-character YouTube video ID from a URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): A YouTube URL containing a video ID.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted video ID or error message if parsing fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Regex pattern to match video IDs\n",
    "    pattern = r'(?:v=|be/|embed/)([a-zA-Z0-9_-]{11})'\n",
    "    match = re.search(pattern, url)\n",
    "    return match.group(1) if match else \"Error: Invalid YouTube URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator wraps your function, adding those attributes (.name, .description, .func) and registering it with LangChain's tool system. The original function becomes accessible through the .func attribute, but the overall object is an instance of LangChain's tool class, with additional methods like .run() for direct invocation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Testing the video ID extraction tool\n",
    "\n",
    "\n",
    "Now you'll be testing your `extract_video_id` tool to verify that it's correctly registered with LangChain. These print statements will show you:\n",
    "1. The tool's name (as it will be referenced by the LLM)\n",
    "2. The tool's description (which helps the LLM understand when to use this tool)\n",
    "3. The actual function reference that will be called\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_video_id\n",
      "----------------------------\n",
      "Extracts the 11-character YouTube video ID from a URL.\n",
      "\n",
      "Args:\n",
      "    url (str): A YouTube URL containing a video ID.\n",
      "\n",
      "Returns:\n",
      "    str: Extracted video ID or error message if parsing fails.\n",
      "----------------------------\n",
      "<function extract_video_id at 0x7d88a38bfec0>\n"
     ]
    }
   ],
   "source": [
    "print(extract_video_id.name)\n",
    "print(\"----------------------------\")\n",
    "print(extract_video_id.description)\n",
    "print(\"----------------------------\")\n",
    "print(extract_video_id.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing tool execution\n",
    "\n",
    "Here, you're testing the actual execution of your `extract_video_id` tool with a real YouTube URL. You can call the tool using the `.run()` method, which is a convenient way to execute the tool directly and see its output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9HODmf-jx58'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_video_id.run(\"https://www.youtube.com/watch?v=9HODmf-jx58&ab_channel=FOXSports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='extract_video_id', description='Extracts the 11-character YouTube video ID from a URL.\\n\\nArgs:\\n    url (str): A YouTube URL containing a video ID.\\n\\nReturns:\\n    str: Extracted video ID or error message if parsing fails.', args_schema=<class 'langchain_core.utils.pydantic.extract_video_id'>, func=<function extract_video_id at 0x7d88a38bfec0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows that your function has been transformed into a `StructuredTool` object by LangChain. It displays the tool's name ('extract_video_id'), its description (our docstring), a Pydantic schema for input validation, and a reference to your original function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool list \n",
    "Multiple tools will be created to enhance the LLM's capabilities. For organization, create a list called tools, which is a standard Python list that contains tool objects created with the @tool decorator. This list doesn't execute functions or determine call order - it simply collects tool objects in one place so they can be efficiently passed to the language model via llm.bind_tools(tools). This approach allows the LLM to access all available tools without requiring them to be individually registered.\n",
    "\n",
    "Adding the ```extract_video_id``` tool to your tools list, which you can later provide to the LLM so it can use this functionality when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = []\n",
    "tools.append(extract_video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have understood the basic structure, let's define the rest of the tools you'll need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining transcript fetching tool\n",
    "\n",
    "Now you're going to create another tool that fetches the transcript from a YouTube video. This tool uses the `YouTubeTranscriptApi` library to retrieve the captions or subtitles from a video. You'll be taking the video ID (which can be extracted using your previous tool) and an optional language parameter. The function attempts to get the transcript and joins all text segments into a continuous string, or returns an error message if the transcript can't be retrieved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_transcript(video_id: str, language: str = \"en\") -> str:\n",
    "    \"\"\"\n",
    "    Fetches the transcript of a YouTube video.\n",
    "    \n",
    "    Args:\n",
    "        video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n",
    "        language (str): Language code for the transcript (e.g., \"en\", \"es\").\n",
    "    \n",
    "    Returns:\n",
    "        str: The transcript text or an error message.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        ytt_api = YouTubeTranscriptApi()\n",
    "        transcript = ytt_api.fetch(video_id, languages=[language])\n",
    "        return \" \".join([snippet.text for snippet in transcript.snippets])\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the fetch_transcript tool by directly calling it with the .run() method on a specific video ID. This will attempt to retrieve the transcript for the video with ID \"hfIUstzHs9A\" in the default English language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Who's ready for the starting lineups? First for the American League, as announced by the manager of the Yankees, Aaron Boon. Leading off from the Detroit Tigers, second baseman Claver Torres. [Applause] [Music] [Applause] Batting second from the Tigers in left field, Riley Green. [Music] [Applause] Batting third from the Yankees in right field, Aaron Judge. Batting fourth from the Seattle Mariners, it's the big dumper, the Derby champ, the catcher, Cal Raleigh. [Music] Betting fifth from the Toronto Blue Jays, first baseman, Vladimir Guerrero Jr. [Applause] Batting six from the Baltimore Orioles, the designated hitter, Ryan O'Harn. Battle seven from the Tampa Bay Rays, the third baseman, Junior Camro. from the Tigers in center field, Javier Bayz. [Music] And batting nine from the Athletics, shortstop, Jacob Wilson. [Applause] Hey, and the starting pitcher for the American League from the Tigers, TKO, Dave Roberts. Yeah. Yeah. [Applause] Leading off from the Dodgers, DH Show Otani. All right, Atlanta. The two hitter in right field, Ronaldo Jr. [Applause] batting third from the Arizona Diamondbacks, the second baseman, Katel Marte. [Music] All right, Atlanta, it's another one of your own. batting for from the Dodgers in first base. Welcome back to Freddy Freeman. In fifth from the San Diego Padres's, the third baseman, Manny Machado. Batting six from the Dodgers in the bullpen, the catcher, Will Smith. Batting seven from the Chicago Cubs in left field, Kyle Tucker. Better from the New York Mets, the shortstop, Mr. Smile. Francisco Indoor and Betty N from the Cubs in center field. PCA Pete Crow Armstrong. Starting the game for the National League from the Pittsburgh Pirates. fans. Let's meet the teams. First from the American League from the Athletics, designated hitter Brett Rooker for the Boston Red Sox. Pitcher Garrett Crochet. Come on. And pitcher Chapman from the Chicago White Socks. pitcher Shane Smith from the Cleveland Guardians. Outfielder Steven Quan from the Detroit Tigers. Pitcher Casey M. And utility man, Zack McKenry from the Houston Astros. Short Stop Jeremy Pñena. Yeah. Pitcher Hunter Brown and pitcher Josh Kater. Yeah. From the Kansas City Royals, shortstop Bobby W Jr., infielder Mike Garcia, pitcher Chris Bubich, and pitcher Carlos Stez. From the Los Angeles Angels, pitcher Usay Kakoshi from the Minnesota Twins, outfielder Byron Fox and pitcher Joe Ryan. From the New York Yankees, infielder Jazz Chisum Jr., Pitcher Max Free and pitcher Carlos Rodan from the Seattle Mariners, pitcher Brian Woo, outfielder Randy Rosarena and pitcher Andre Munoz from the Tampa Bay Rays. Second baseman Brandon Wow. First baseman Jonathan Aronda and pitcher Drew Rasmus from the Texas Rangers. Pitcher Jacob Deg Gro from the Toronto Blue Jays. Catcher Alejandro Kirk. The honorary coaches for the American League. From the Cleveland Guardians, the manager Steven V. And from the New York Yankees, please welcome Joe Tory. [Applause] Oh. Oh. And now the National League. Oh my god. First up from the Arizona Diamondbacks, outfielder Corbin Carroll. Third baseman Suarez. All right, your Atlanta Braves first baseman. Yeah. Yeah. Yeah. Yeah. And pitcher Chris S [Applause] from the Chicago Cubs. Pitcher Matthew Boyd from the Cincinnati Reds. Pitcher Andrew Abbott and shortstop Ellie Delight Cruz [Applause] from the Colorado Rockies. Catcher Hunter Goodman from the Los Angeles Dodgers. Pitcher Yoshi Nou Yamamoto. and warming up, getting ready to pitch early in this game. Back at the All-Star game, Clayton Kershaw from the Miami Marlins. Outfielder Kyle Sters from the Milwaukee Brewers. The Miz Jacob Miserowski. Pitcher Freddy Peralta. And pitcher Trevor McIll from the New York Mets. First baseman Pete Alonzo, pitcher David Peterson, and pitcher Edwin Diaz from the Philadelphia Phillies, DH Kyle Schwber from the San Diego Padres's Outfielder Fernando Tatis Jr. Pitcher Jason Adams, pitcher Adrien Lauron and pitcher Robert Suarez. From the San Francisco Giants, pitcher Robbie Ray, pitcher Logan Webb and pitcher Randy Rodriguez from the St. Louis Cardinals. Second baseman Brendan Donovan from the Washington Nationals, pitcher McKenzie Gore and outfielder James Wood. And your honorary coaches from the Marlins, Clayton McCulla and the skipper Braves, Brian Smith. [Applause] Let's hear it for your 2025 Allstars.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_transcript.run(\"9HODmf-jx58\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Adding the `fetch_transcript` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(fetch_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining YouTube search tool\n",
    "\n",
    "Now let's create a search tool that allows finding videos on YouTube based on a query string. This tool uses the `Search` class from the PyTube library to perform searches on YouTube. When given a search term, it returns a list of matching videos with each video represented as a dictionary containing the title, video ID, and a shortened URL. This tool will be helpful for discovering relevant videos when you don't already have a specific URL in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import Search\n",
    "from langchain.tools import tool\n",
    "from typing import List, Dict\n",
    "\n",
    "@tool\n",
    "def search_youtube(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Search YouTube for videos matching the query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search term to look for on YouTube\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing video titles and IDs in format:\n",
    "        [{'title': 'Video Title', 'video_id': 'abc123'}, ...]\n",
    "        Returns error message if search fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = Search(query)\n",
    "        return [\n",
    "            {\n",
    "                \"title\": yt.title,\n",
    "                \"video_id\": yt.video_id,\n",
    "                \"url\": f\"https://youtu.be/{yt.video_id}\"\n",
    "            }\n",
    "            for yt in s.results\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `search_youtube` tool by calling it with the `.run()` method and the search query \"Generative AI.\" This will return a list of YouTube videos related to generative AI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "title": "Kobe Bryant’s Thoughts On LeBrons Taco Tuesday? #nba #basketball #kobebryant",
        "url": "https://youtu.be/AAY3qs47rl8",
        "video_id": "AAY3qs47rl8"
       },
       {
        "title": "Jeff Teague tells story of Joe Johnson COOKING Kobe Bryant in Hawks-Lakers | Club 520",
        "url": "https://youtu.be/J4fyjar7EZo",
        "video_id": "J4fyjar7EZo"
       },
       {
        "title": "Unc & Ocho FIRED UP Debating Kobe Bryant Being Left Out of Top 10 All-Time NBA Players! | Nightcap",
        "url": "https://youtu.be/2u4h745goZk",
        "video_id": "2u4h745goZk"
       },
       {
        "title": "Why Kobe Bryant MADE Body Armor 😳🥤",
        "url": "https://youtu.be/TurBLeZVifc",
        "video_id": "TurBLeZVifc"
       },
       {
        "title": "Bleacher Reports Garbage List Slanders Kobe Bryant",
        "url": "https://youtu.be/UvRKXWvafmc",
        "video_id": "UvRKXWvafmc"
       },
       {
        "title": "Kobe Bryant x Nike Air Force 1 \"Forever White\" On Feet",
        "url": "https://youtu.be/pdCReenj1w8",
        "video_id": "pdCReenj1w8"
       },
       {
        "title": "Gil's Arena Debates What If Kobe Played Today",
        "url": "https://youtu.be/VNU76IPGg8E",
        "video_id": "VNU76IPGg8E"
       },
       {
        "title": "Practice With Kobe Bryant Was Intense😅😳",
        "url": "https://youtu.be/ClHYFZgNPdU",
        "video_id": "ClHYFZgNPdU"
       },
       {
        "title": "Kobe Bryant ULTIMATE Career Mixtape!",
        "url": "https://youtu.be/BaTd_F2yIrU",
        "video_id": "BaTd_F2yIrU"
       },
       {
        "title": "KOBE BRYANT'S LAST GREAT INTERVIEW On How To FIND PURPOSE In LIFE | Kobe Bryant & Jay Shetty",
        "url": "https://youtu.be/g2cQ2kD6lzs",
        "video_id": "g2cQ2kD6lzs"
       },
       {
        "title": "Kobe Bryant CHAMPION MINDSET - What Separates the WINNERS from the LOSERS (MUST WATCH)",
        "url": "https://youtu.be/Ju5kyQJyGBY",
        "video_id": "Ju5kyQJyGBY"
       },
       {
        "title": "Kobe Bryant's TOP 40 Plays of His NBA Career!",
        "url": "https://youtu.be/1fjhIWJSxfw",
        "video_id": "1fjhIWJSxfw"
       },
       {
        "title": "THE MINDSET OF A WINNER | Kobe Bryant Champions Advice",
        "url": "https://youtu.be/VSceuiPBpxY",
        "video_id": "VSceuiPBpxY"
       },
       {
        "title": "The World’s GREATEST Kobe Bryant Highlight Reel 🐍",
        "url": "https://youtu.be/sJXWscXF3t4",
        "video_id": "sJXWscXF3t4"
       },
       {
        "title": "Every Version Of Kobe Bryant NBA Tier List! 🏀👀 #shorts",
        "url": "https://youtu.be/arugzXcUuxk",
        "video_id": "arugzXcUuxk"
       },
       {
        "title": "NBA Legends Explain Why Kobe Bryant Was The Best Trash Talker",
        "url": "https://youtu.be/Hj3BAoTg-wg",
        "video_id": "Hj3BAoTg-wg"
       },
       {
        "title": "Kobe Bryant - Inspirational Video",
        "url": "https://youtu.be/JHVMW3U5BZE",
        "video_id": "JHVMW3U5BZE"
       },
       {
        "title": "Kobe Bryant knew Gianna had the Mamba gene 🐍",
        "url": "https://youtu.be/Q3s6gFIE6dk",
        "video_id": "Q3s6gFIE6dk"
       },
       {
        "title": "Kobe Bryant’s Last Great Interview",
        "url": "https://youtu.be/T9GvDekiJ9c",
        "video_id": "T9GvDekiJ9c"
       },
       {
        "title": "How Kobe Bryant was UNSTOPPABLE🐐",
        "url": "https://youtu.be/BAddlt5j_QE",
        "video_id": "BAddlt5j_QE"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_out=search_youtube.run(\"Kobe Bryant\")\n",
    "display(JSON(search_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending the `search_youtube` tool to tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(search_youtube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining metadata extraction tool\n",
    "\n",
    "Now you'll create a tool that extracts detailed metadata from a YouTube video using the `yt-dlp` library. This tool takes a YouTube URL and returns comprehensive information about the video, including its title, view count, duration, channel name, like count, comment count, and any chapter markers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_full_metadata(url: str) -> dict:\n",
    "    \"\"\"Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.\"\"\"\n",
    "    with yt_dlp.YoutubeDL({'quiet': True, 'logger': yt_dpl_logger}) as ydl:\n",
    "        info = ydl.extract_info(url, download=False)\n",
    "        return {\n",
    "            'title': info.get('title'),\n",
    "            'views': info.get('view_count'),\n",
    "            'duration': info.get('duration'),\n",
    "            'channel': info.get('uploader'),\n",
    "            'likes': info.get('like_count'),\n",
    "            'comments': info.get('comment_count'),\n",
    "            'chapters': info.get('chapters', [])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_full_metadata` tool by running it on a specific YouTube video URL. This will extract comprehensive information about the video with ID \"qWHaMrR5WHQ\" without downloading the actual video content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "channel": "LetsLearnWithChinnoVino",
       "chapters": null,
       "comments": 5,
       "duration": 1048,
       "likes": 22,
       "title": "Explained how to use Tools (tool calling) in LangChain",
       "views": 1308
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_data=get_full_metadata.run(\"https://youtu.be/qWHaMrR5WHQ\")\n",
    "display(JSON(meta_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the `get_full_metadata` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_full_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining trending videos tool\n",
    "\n",
    "Now, you'll create a tool to fetch the currently trending videos on YouTube for a specific region. This tool uses `yt-dlp` to access YouTube's trending feed based on a provided country code (like \"US\" for the United States or \"IN\" for India). It collects important information about each trending video, including the title, video ID, URL, channel name, duration, and view count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "@tool\n",
    "def get_trending_videos(region_code: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches currently trending YouTube videos for a specific region.\n",
    "    \n",
    "    Args:\n",
    "        region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with video details: title, video_id, channel, view_count, duration\n",
    "    \"\"\"\n",
    "    ydl_opts = {\n",
    "        'geo_bypass_country': region_code.upper(),\n",
    "        'extract_flat': True,\n",
    "        'quiet': True,\n",
    "        'force_generic_extractor': True,\n",
    "        'logger': yt_dpl_logger\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            info = ydl.extract_info(\n",
    "                'https://www.youtube.com/feed/trending',\n",
    "                download=False\n",
    "            )\n",
    "            \n",
    "            trending_videos = []\n",
    "            for entry in info['entries']:\n",
    "                video_data = {\n",
    "                    'title': entry.get('title', 'N/A'),\n",
    "                    'video_id': entry.get('id', 'N/A'),\n",
    "                    'url': entry.get('url', 'N/A'),\n",
    "                    'channel': entry.get('uploader', 'N/A'),\n",
    "                    'duration': entry.get('duration', 0),\n",
    "                    'view_count': entry.get('view_count', 0)\n",
    "                }\n",
    "                trending_videos.append(video_data)\n",
    "                \n",
    "            return trending_videos[:25]  # Return top 25 trending videos\n",
    "            \n",
    "    except Exception as e:\n",
    "        return [{'error': f\"Failed to fetch trending videos: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_trending_videos` tool by running it with the region code `\"US\"` to fetch trending videos from the United States. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "channel": "MLB",
        "duration": 1258,
        "title": "2025 MLB All-Star Game Full Game Highlights (7/15/25) | MLB Highlights",
        "url": "https://www.youtube.com/watch?v=Ykqhonhqios",
        "video_id": "Ykqhonhqios",
        "view_count": 837181
       },
       {
        "channel": "Vsauce",
        "duration": 1320,
        "title": "All The Ghosts You Will Be",
        "url": "https://www.youtube.com/watch?v=xHd4zsIbXJ0",
        "video_id": "xHd4zsIbXJ0",
        "view_count": 1472167
       },
       {
        "channel": "America's Got Talent",
        "duration": 520,
        "title": "Jessica Sanchez Receives A GOLDEN BUZZER From Sofia Vergara | AGT 2025",
        "url": "https://www.youtube.com/watch?v=a-V1XTvsPq8",
        "video_id": "a-V1XTvsPq8",
        "view_count": 2195334
       },
       {
        "channel": "Jake Webber",
        "duration": 3430,
        "title": "We went to Italy!",
        "url": "https://www.youtube.com/watch?v=2lMAy9ga7nU",
        "video_id": "2lMAy9ga7nU",
        "view_count": 276051
       },
       {
        "channel": "theneedledrop",
        "duration": 1283,
        "title": "Clipse - Let God Sort Em Out ALBUM REVIEW",
        "url": "https://www.youtube.com/watch?v=h-aYBc_xXDU",
        "video_id": "h-aYBc_xXDU",
        "view_count": 446924
       },
       {
        "channel": "Joe and Jada",
        "duration": 3304,
        "title": "Clipse on \"Let God Sort Em Out,” Pharrell, Jay-Z & Def Jam | Fat Joe, Jadakiss, Pusha T & Malice",
        "url": "https://www.youtube.com/watch?v=QZjdxk5tsFI",
        "video_id": "QZjdxk5tsFI",
        "view_count": 426800
       },
       {
        "channel": "America's Got Talent",
        "duration": 619,
        "title": "Anna Wilson Has No Idea She's About To Audition... And Then She NAILS It! | AGT 2025",
        "url": "https://www.youtube.com/watch?v=hY5YV57kVbg",
        "video_id": "hY5YV57kVbg",
        "view_count": 917412
       },
       {
        "channel": "Street Fighter",
        "duration": 165,
        "title": "Street Fighter 6 - Sagat Gameplay Trailer",
        "url": "https://www.youtube.com/watch?v=sBNCSeG391Y",
        "video_id": "sBNCSeG391Y",
        "view_count": 814630
       },
       {
        "channel": "Joyner Lucas",
        "duration": 189,
        "title": "Joyner Lucas - NOBODY CARES",
        "url": "https://www.youtube.com/watch?v=sr_XMq9TkGA",
        "video_id": "sr_XMq9TkGA",
        "view_count": 427252
       },
       {
        "channel": "FOX Sports",
        "duration": 673,
        "title": "2025 MLB All-Star Game: Full Home Run swing-off | MLB on FOX",
        "url": "https://www.youtube.com/watch?v=8lksBr4rqNw",
        "video_id": "8lksBr4rqNw",
        "view_count": 93876
       },
       {
        "channel": "Glorb",
        "duration": 221,
        "title": "Glorb - GHOST IN THE SHELL (Official Music Video)",
        "url": "https://www.youtube.com/watch?v=AlPv2DAiJM0",
        "video_id": "AlPv2DAiJM0",
        "view_count": 201847
       },
       {
        "channel": "Airrack",
        "duration": 1965,
        "title": "I Secretly Hid In MrBeast's YouTube Videos",
        "url": "https://www.youtube.com/watch?v=On7He5aG9jg",
        "video_id": "On7He5aG9jg",
        "view_count": 5570753
       },
       {
        "channel": "The Act Man",
        "duration": 1521,
        "title": "The Painful Death of Xbox",
        "url": "https://www.youtube.com/watch?v=E2BYCu-SZSY",
        "video_id": "E2BYCu-SZSY",
        "view_count": 422710
       },
       {
        "channel": "WWE",
        "duration": 192,
        "title": "The OTC IS BACK | Roman Reigns makes epic return: Raw, July 14, 2025",
        "url": "https://www.youtube.com/watch?v=3rJzDnZD4Ys",
        "video_id": "3rJzDnZD4Ys",
        "view_count": 3298345
       },
       {
        "channel": "Travis Scott",
        "duration": 120,
        "title": "Travis Scott - KICK OUT",
        "url": "https://www.youtube.com/watch?v=EBr7YTNBzoM",
        "video_id": "EBr7YTNBzoM",
        "view_count": 2190108
       },
       {
        "channel": "Druski",
        "duration": 179,
        "title": "Coulda Been House Season 2 Official Trailer",
        "url": "https://www.youtube.com/watch?v=Y2kZeukW2xM",
        "video_id": "Y2kZeukW2xM",
        "view_count": 490789
       },
       {
        "channel": "Clipse",
        "duration": 282,
        "title": "Clipse, Kendrick Lamar, Pusha T, Malice - Chains & Whips (Official Music Video)",
        "url": "https://www.youtube.com/watch?v=ecIH-4RbbOk",
        "video_id": "ecIH-4RbbOk",
        "view_count": 1938225
       },
       {
        "channel": "Kalogeras Sisters ",
        "duration": 3067,
        "title": "Kalogeras Sisters SQUID GAME IN REAL LIFE!",
        "url": "https://www.youtube.com/watch?v=s90P0wBjID8",
        "video_id": "s90P0wBjID8",
        "view_count": 3065695
       },
       {
        "channel": "Rick Beato",
        "duration": 670,
        "title": "I'm Sorry...This New Artist Completely Sucks",
        "url": "https://www.youtube.com/watch?v=eKxNGFjyRv0",
        "video_id": "eKxNGFjyRv0",
        "view_count": 727220
       },
       {
        "channel": "Louis BPM",
        "duration": 217,
        "title": "Louis BPM , Hanzel La H, Junior Caldera - Esto Es Asi (Video Oficial)",
        "url": "https://www.youtube.com/watch?v=ap8hweDLGmQ",
        "video_id": "ap8hweDLGmQ",
        "view_count": 205662
       },
       {
        "channel": "SSundee",
        "duration": 1042,
        "title": "Using ADMIN SERVER COMMANDS in Roblox Steal a Brainrot",
        "url": "https://www.youtube.com/watch?v=zXX5RLZP7Oo",
        "video_id": "zXX5RLZP7Oo",
        "view_count": 711774
       },
       {
        "channel": "EminemMusic",
        "duration": 43,
        "title": "STANS (In Theatres August 7)",
        "url": "https://www.youtube.com/watch?v=DZkDackH-H8",
        "video_id": "DZkDackH-H8",
        "view_count": 420369
       },
       {
        "channel": "Royal Court",
        "duration": 1515,
        "title": "David Corenswet Joins Brittany Broski's Royal Court",
        "url": "https://www.youtube.com/watch?v=HSkxQslmZ8Q",
        "video_id": "HSkxQslmZ8Q",
        "view_count": 754178
       },
       {
        "channel": "Seventeen",
        "duration": 509,
        "title": "The 'Zombies 4' Cast Talk Co-Star Crushes & Cringe Auditions | Pass the Popcorn | Seventeen",
        "url": "https://www.youtube.com/watch?v=dmYHOKWXUGg",
        "video_id": "dmYHOKWXUGg",
        "view_count": 164773
       },
       {
        "channel": "Jaime French",
        "duration": 1122,
        "title": "The Labubu Cult has Lost It",
        "url": "https://www.youtube.com/watch?v=HqhlAk1GqCE",
        "video_id": "HqhlAk1GqCE",
        "view_count": 139766
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trending_videos=get_trending_videos.run(\"US\")\n",
    "# Display as formatted JSON\n",
    "display(JSON(trending_videos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the `get_trending_videos` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_trending_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining thumbnail retrieval tool\n",
    "\n",
    "Now you'll create a tool to extract all available thumbnail images for a YouTube video. This tool uses `yt-dlp` to retrieve information about the various thumbnail images that YouTube generates for videos at different resolutions. For each thumbnail, collect its URL, width, height, and formatted resolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_thumbnails(url: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Get available thumbnails for a YouTube video using its URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): YouTube video URL (any format)\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with thumbnail URLs and resolutions in YouTube's native order\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL({'quiet': True, 'logger': yt_dpl_logger}) as ydl:\n",
    "            info = ydl.extract_info(url, download=False)\n",
    "            \n",
    "            thumbnails = []\n",
    "            for t in info.get('thumbnails', []):\n",
    "                if 'url' in t:\n",
    "                    thumbnails.append({\n",
    "                        \"url\": t['url'],\n",
    "                        \"width\": t.get('width'),\n",
    "                        \"height\": t.get('height'),\n",
    "                        \"resolution\": f\"{t.get('width', '')}x{t.get('height', '')}\".strip('x')\n",
    "                    })\n",
    "            \n",
    "            return thumbnails\n",
    "\n",
    "    except Exception as e:\n",
    "        return [{\"error\": f\"Failed to get thumbnails: {str(e)}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll test your `get_thumbnails` tool by running it on a specific YouTube video URL. This will extract information about all available thumbnail images for the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mq1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mq1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sd1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sd1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/default.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/default.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/mqdefault.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/mqdefault.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/0.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/0.webp",
        "width": null
       },
       {
        "height": 94,
        "resolution": "168x94",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLDK33nNmNAuVa1ibN7uTkq5okdwfw",
        "width": 168
       },
       {
        "height": 110,
        "resolution": "196x110",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEbCMQBEG5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLBa9uAOnYdtfae6yOMFyYfUFEvjkA",
        "width": 196
       },
       {
        "height": 138,
        "resolution": "246x138",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEcCPYBEIoBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLDjZI5FIYKE2PlZhm9PQTk0eKeDAw",
        "width": 246
       },
       {
        "height": 188,
        "resolution": "336x188",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg?sqp=-oaymwEcCNACELwBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLBTNOqMuqjK5oJ-_mtGsUJOh-sVkA",
        "width": 336
       },
       {
        "height": 360,
        "resolution": "480x360",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hqdefault.jpg",
        "width": 480
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hqdefault.webp",
        "width": null
       },
       {
        "height": 480,
        "resolution": "640x480",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/sddefault.jpg",
        "width": 640
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/sddefault.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/hq720.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/hq720.webp",
        "width": null
       },
       {
        "height": 720,
        "resolution": "1280x720",
        "url": "https://i.ytimg.com/vi/qWHaMrR5WHQ/maxresdefault.jpg",
        "width": 1280
       },
       {
        "height": 1080,
        "resolution": "1920x1080",
        "url": "https://i.ytimg.com/vi_webp/qWHaMrR5WHQ/maxresdefault.webp",
        "width": 1920
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "thumbnails=get_thumbnails.run(\"https://www.youtube.com/watch?v=qWHaMrR5WHQ\")\n",
    "\n",
    "display(JSON(thumbnails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the `get_thumbnails` tool to your tools list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.append(get_thumbnails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Binding tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, you'll bind your collection of tools to the language model. It enables the LLM to access and use your custom YouTube tools during conversations. By binding the tools, you're giving the model the ability to call these functions when it determines they're needed to fulfill a user request, making the LLM aware of your tools' capabilities and how to use them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```bind_tools()``` function passes all this information to the language model. It converts each tool's attributes (name, description, parameters schema) into a standardized format that the LLM can understand and use to determine when and how to call specific tools based on user requests. Similar to the following code where the schema for each tool is stored:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "description": "Extracts the 11-character YouTube video ID from a URL.\n\nArgs:\n    url (str): A YouTube URL containing a video ID.\n\nReturns:\n    str: Extracted video ID or error message if parsing fails.",
       "name": "extract_video_id",
       "parameters": {
        "description": "Extracts the 11-character YouTube video ID from a URL.\n\nArgs:\n    url (str): A YouTube URL containing a video ID.\n\nReturns:\n    str: Extracted video ID or error message if parsing fails.",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "extract_video_id",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Fetches the transcript of a YouTube video.\n\nArgs:\n    video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n    language (str): Language code for the transcript (e.g., \"en\", \"es\").\n\nReturns:\n    str: The transcript text or an error message.",
       "name": "fetch_transcript",
       "parameters": {
        "description": "Fetches the transcript of a YouTube video.\n\nArgs:\n    video_id (str): The YouTube video ID (e.g., \"dQw4w9WgXcQ\").\n    language (str): Language code for the transcript (e.g., \"en\", \"es\").\n\nReturns:\n    str: The transcript text or an error message.",
        "properties": {
         "language": {
          "default": "en",
          "title": "Language",
          "type": "string"
         },
         "video_id": {
          "title": "Video Id",
          "type": "string"
         }
        },
        "required": [
         "video_id"
        ],
        "title": "fetch_transcript",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.",
       "name": "get_full_metadata",
       "parameters": {
        "description": "Extract metadata given a YouTube URL, including title, views, duration, channel, likes, comments, and chapters.",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "get_full_metadata",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Fetches currently trending YouTube videos for a specific region.\n\nArgs:\n    region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n\nReturns:\n    List of dictionaries with video details: title, video_id, channel, view_count, duration",
       "name": "get_trending_videos",
       "parameters": {
        "description": "Fetches currently trending YouTube videos for a specific region.\n\nArgs:\n    region_code (str): 2-letter country code (e.g., \"US\", \"IN\", \"GB\")\n\nReturns:\n    List of dictionaries with video details: title, video_id, channel, view_count, duration",
        "properties": {
         "region_code": {
          "title": "Region Code",
          "type": "string"
         }
        },
        "required": [
         "region_code"
        ],
        "title": "get_trending_videos",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "description": "Get available thumbnails for a YouTube video using its URL.\n\nArgs:\n    url (str): YouTube video URL (any format)\n\nReturns:\n    List of dictionaries with thumbnail URLs and resolutions in YouTube's native order",
       "name": "get_thumbnails",
       "parameters": {
        "description": "Get available thumbnails for a YouTube video using its URL.\n\nArgs:\n    url (str): YouTube video URL (any format)\n\nReturns:\n    List of dictionaries with thumbnail URLs and resolutions in YouTube's native order",
        "properties": {
         "url": {
          "title": "Url",
          "type": "string"
         }
        },
        "required": [
         "url"
        ],
        "title": "get_thumbnails",
        "type": "object"
       },
       "return": null
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for tool in tools:\n",
    "    schema = {\n",
    "   \"name\": tool.name,\n",
    "   \"description\": tool.description,\n",
    "   \"parameters\": tool.args_schema.schema() if tool.args_schema else {},\n",
    "   \"return\": tool.return_type if hasattr(tool, \"return_type\") else None}\n",
    "    display(JSON(schema))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the LLM calls a tool\n",
    "\n",
    "Now, define a sample user query that asks for a summary of a specific YouTube video. This query will be used to demonstrate how your LLM can understand a natural language request and use the appropriate tools you've provided to fulfill it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\n"
     ]
    }
   ],
   "source": [
    "query = \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating a message object to represent your user query. You'll be wrapping the query string in a HumanMessage object, which is the standard way to format user inputs in LangChain. It represents a human message as a person is expected to initiate the interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content = query)]\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain tool binding process\n",
    "\n",
    "This step involves sending your message to the LLM and storing its response. Here you'll invoke the language model with your user query about summarizing a YouTube video. The response will contain both text content and potentially tool calls that the model decides to make. ``response_1`` contains the LLM's response to the user message, including any tool calls it decides to make. The response object contains the content of the LLM's reply plus structured information about which tools it wants to call and with what parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_33D4YyftQVrMIOWSlsANYRr1', 'function': {'arguments': '{\"url\":\"https://www.youtube.com/watch?v=T-D1OfcDW1M\"}', 'name': 'extract_video_id'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 379, 'total_tokens': 408, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': None, 'id': 'chatcmpl-Bu0H9h34j2rTgTLBUskq1G4FuWwNI', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ff80a298-3ba7-471e-a277-a62b77a0af71-0', tool_calls=[{'name': 'extract_video_id', 'args': {'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'}, 'id': 'call_33D4YyftQVrMIOWSlsANYRr1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 379, 'output_tokens': 29, 'total_tokens': 408, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1 = llm_with_tools.invoke(messages)\n",
    "response_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the LLM's response to your conversation history. After receiving the response from the language model (which contains the tool call to extract the video ID), append it to your messages list to maintain the conversation context. This builds up the chat history that will be used for subsequent interactions with the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting tool call information\n",
    "After receiving the LLM's response, you need to extract the structured tool call information. The line tool_calls_1 = response_1.tool_calls gets the tool call objects that contain which tool the LLM has decided to use and what parameters to pass to it. This information will be used to execute the appropriate tool with the correct inputs.\n",
    "\n",
    "#### Creating a tool mapping dictionary\n",
    "\n",
    "Now you'll create a dictionary that maps tool names to their corresponding function objects. This mapping will be useful later when you need to programmatically invoke specific tools based on their names. It allows you to easily look up and execute a tool function when you have only the tool name as a string, which will be important when processing tool calls from the language model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_mapping = {\n",
    "    \"get_thumbnails\" : get_thumbnails,\n",
    "    \"get_trending_videos\": get_trending_videos,\n",
    "    \"extract_video_id\": extract_video_id,\n",
    "    \"fetch_transcript\": fetch_transcript,\n",
    "    \"search_youtube\": search_youtube,\n",
    "    \"get_full_metadata\": get_full_metadata\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the tool calls from the language model's response. When the LLM determines it needs to use one of your tools, it includes structured \"tool_calls\" in its response. Here, you're accessing those tool calls to see which tools the model decided to use in order to fulfill the request about summarizing the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "args": {
         "url": "https://www.youtube.com/watch?v=T-D1OfcDW1M"
        },
        "id": "call_33D4YyftQVrMIOWSlsANYRr1",
        "name": "extract_video_id",
        "type": "tool_call"
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls_1 = response_1.tool_calls\n",
    "display(JSON(tool_calls_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you're seeing the structure of the tool call that the LLM decided to make. The tool call is formatted as a dictionary with the following key components:\n",
    "\n",
    "1. `name`: 'extract_video_id' - This identifies which tool the LLM wants to use first (the video ID extraction tool)\n",
    "2. `args`: Contains the arguments to pass to the tool - in this case, the YouTube URL from your query\n",
    "3. `id`: A unique identifier for this specific tool call, which helps track the request/response pair\n",
    "4. `type`: Indicates this is a tool call rather than other types of AI responses\n",
    "\n",
    "This shows that the LLM correctly understood it needs to first extract the video ID from the URL before it can proceed with summarizing the video content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the name of the first tool that the LLM decided to use. Here you're extracting just the name component `('extract_video_id')` from the first tool call in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_video_id\n"
     ]
    }
   ],
   "source": [
    "tool_name=tool_calls_1[0]['name']\n",
    "print(tool_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need a tool ID to help the LLM know where the output came from:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call_33D4YyftQVrMIOWSlsANYRr1\n"
     ]
    }
   ],
   "source": [
    "tool_call_id =tool_calls_1[0]['id']\n",
    "print(tool_call_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the arguments that need to be passed to the chosen tool. Here, you're extracting the arguments component from the first tool call, which contains the YouTube URL that needs to be processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.youtube.com/watch?v=T-D1OfcDW1M'}\n"
     ]
    }
   ],
   "source": [
    "args=tool_calls_1[0]['args']\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the LLM's response to your conversation history. After receiving the response from the language model (which contains the tool call to extract the video ID), you append it to your messages list to maintain the conversation context. This builds up the chat history that will be used for subsequent interactions with the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the tool call that the LLM requested. Here, you're using your tool mapping dictionary to:\n",
    "1. Look up the appropriate function based on the tool name ('extract_video_id')\n",
    "2. Call that function with the arguments provided by the LLM\n",
    "3. Capture the output (the extracted video ID)\n",
    "\n",
    "This shows how you can programmatically execute the tools that the LLM decided to use. First, you get the tool from ```tool_mapping```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tool=tool_mapping[tool_calls_1[0]['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll then call the tool with the arguments:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T-D1OfcDW1M'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id =my_tool.invoke(tool_calls_1[0]['args'])\n",
    "video_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the tool's output to your conversation history. You'll create a `ToolMessage` that contains:\n",
    "1. The result from executing the tool (the extracted video ID)\n",
    "2. The original tool call ID to link this response back to the specific request\n",
    "\n",
    "By appending this message to your conversation history, you're informing the LLM about the results of the tool execution, which it can use in its next response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ToolMessage(content = video_id, tool_call_id = tool_calls_1[0]['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send your updated conversation to the LLM and store its new response. Now that you've informed the model about the extracted video ID, invoke it again to continue the process. The model will see both the original query and the result of the video ID extraction, allowing it to determine the next step needed to summarize the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZXUrZNxaMYB7dB5U6BvcA9hj', 'function': {'arguments': '{\"video_id\":\"T-D1OfcDW1M\",\"language\":\"en\"}', 'name': 'fetch_transcript'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 425, 'total_tokens': 452, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': None, 'id': 'chatcmpl-Bu19cowUYBkctwrHGYyCL8vFb2pfy', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--59d76ba6-cc07-4189-9e9c-199cc225b13a-0', tool_calls=[{'name': 'fetch_transcript', 'args': {'video_id': 'T-D1OfcDW1M', 'language': 'en'}, 'id': 'call_ZXUrZNxaMYB7dB5U6BvcA9hj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 425, 'output_tokens': 27, 'total_tokens': 452, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_2 = llm_with_tools.invoke(messages)\n",
    "response_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a AI messege! Send your updated conversation to the LLM and store its new response. Now that you've informed the model about the extracted video ID, you'll invoke it again to continue the process. The model will see both the original query and the result of the video ID extraction, allowing it to determine the next step needed to summarize the YouTube video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the tool calls from the language model's second response. After receiving the video ID, the LLM will likely decide to use another tool to help with the summarization task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'fetch_transcript',\n",
       "  'args': {'video_id': 'T-D1OfcDW1M', 'language': 'en'},\n",
       "  'id': 'call_ZXUrZNxaMYB7dB5U6BvcA9hj',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls_2 = response_2.tool_calls\n",
    "tool_calls_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can see that the LLM has decided to use the `fetch_transcript` tool as its next step. \n",
    "\n",
    "The model is passing two arguments to the transcript fetching tool:\n",
    "1. `video_id`: 'T-D1OfcDW1M' - The ID that was extracted from the original YouTube URL\n",
    "2. `language`: 'en' - Requesting the transcript in English as specified in the user's query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fetching the transcript using the video ID obtained in the previous step. Here, you're executing the second tool that the LLM requested by:\n",
    "1. Looking up the appropriate function `('fetch_transcript')` from your tool mapping\n",
    "2. Invoking it with the video ID and language parameters\n",
    "3. Storing the resulting transcript content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models. They are everywhere. They get some things amazingly right and other things very interestingly wrong. My name\\xa0is Marina Danilevsky. I am a Senior Research Scientist here at IBM Research. And I want\\xa0to tell you about a framework to help large language models be more accurate and more up to\\xa0date: Retrieval-Augmented Generation, or RAG. Let\\'s just talk about the \"Generation\" part for a\\xa0minute. So forget the \"Retrieval-Augmented\". So the\\xa0generation, this refers to large language models,\\xa0or LLMs, that generate text in response to a user query, referred to as a prompt. These\\xa0models can have some undesirable behavior. I want to tell you an anecdote to illustrate this. So my kids, they recently asked me this question: \"In our solar system, what planet has the most\\xa0moons?\" And my response was, “Oh, that\\'s really great that you\\'re asking this question. I loved\\xa0space when I was your age.” Of course, that was like 30 years ago. But I know this! I read an\\xa0article and the article said that it was Jupiter and 88 moons. So that\\'s the answer. Now, actually,\\xa0there\\'s a couple of things wrong with my answer. First of all, I have no source to support what\\xa0I\\'m saying. So even though I confidently said “I read an article, I know the answer!”, I\\'m not\\xa0sourcing it. I\\'m giving the answer off the top of my head. And also, I actually haven\\'t kept up with\\xa0this for awhile, and my answer is out of date. So we have two problems here. One is no source.\\xa0And the second problem is that I am out of date.\\xa0\\xa0 And these, in fact, are two behaviors that are\\xa0often observed as problematic when interacting with large language models. They’re LLM\\xa0challenges. Now, what would have happened if I\\'d taken a beat and first gone and looked\\xa0up the answer on a reputable source like NASA? Well, then I would have been able to say, “Ah,\\xa0okay! So the answer is Saturn with 146 moons.” And in fact, this keeps changing because scientists\\xa0keep on discovering more and more moons. So I have now grounded my answer in something more\\xa0\\nbelievable. I have not hallucinated or made up an answer. Oh, by the way, I didn\\'t leak personal\\xa0information about how long ago it\\'s been since I was obsessed with space. All right, so what does\\xa0this have to do with large language models? Well, how would a large language model have answered\\xa0this question? So let\\'s say that I have a user asking this question about moons. A large language\\xa0model would confidently say, OK, I have been trained and from what I know in my parameters\\xa0during my training, the answer is Jupiter. The answer is wrong. But, you know, we don\\'t know. The large language model is very confident in what it answered. Now, what happens when you add this\\xa0retrieval augmented part here? What does that mean? That means that now, instead of just relying\\xa0on what the LLM knows, we are adding a content store. This could be open like the internet. This\\xa0can be closed like some collection of documents, collection of policies, whatever. The point,\\xa0though, now is that the LLM first goes and talks to the content store and says,\\xa0“Hey, can you retrieve for me information that is relevant to what the user\\'s\\xa0query was?” And now, with this retrieval-augmented answer, it\\'s not Jupiter anymore. We know that\\xa0it is Saturn. What does this look like? Well, first user prompts the LLM\\xa0with their question. They say, this is what my question was. And originally,\\xa0if we\\'re just talking to a generative model, the generative model says, “Oh, okay, I know\\xa0the response. Here it is. Here\\'s my response.”\\xa0\\xa0 But now in the RAG framework, the generative\\xa0model actually has an instruction that says, \"No, no, no.\" \"First, go and retrieve\\xa0relevant content.\" \"Combine that with the user\\'s question and only then generate the\\xa0answer.\" So the prompt now has three parts: the instruction to pay attention to, the retrieved\\xa0content, together with the user\\'s question. Now give a response. And in fact, now you can give\\xa0evidence for why your response was what it was.\\xa0\\xa0 So now hopefully you can see, how does RAG help the two LLM challenges that I had mentioned before?\\xa0\\xa0 So first of all, I\\'ll start with the out of\\xa0date part. Now, instead of having to retrain your model, if new information comes up, like, hey,\\xa0we found some more moons-- now to Jupiter again, maybe it\\'ll be Saturn again in the future. All\\xa0you have to do is you augment your data store with new information, update information. So now the next time that a user comes and asks the question, we\\'re ready. We just go ahead and retrieve the most up to date information. The second problem, source. Well, the large language model is now being instructed to pay attention to primary source data before giving its response. And in fact, now being able to give evidence. This makes it less likely to hallucinate or to leak data because it is less likely to rely only on information that it learned during training. It also allows us to get the model to have a behavior that can be very positive, which is knowing when to say, “I don\\'t know.” If\\xa0the user\\'s question cannot be reliably answered based on your data store, the model should say,\\xa0\"I don\\'t know,\" instead of making up something that is believable and may mislead the user. This\\xa0can have a negative effect as well though, because if the retriever is not sufficiently\\xa0good to give the large language model the best, most high-quality grounding information, then\\xa0maybe the user\\'s query that is answerable doesn\\'t get an answer. So this is actually why lots\\xa0of folks, including many of us here at IBM, are working the problem on both sides. We are both\\xa0working to improve the retriever to give the large language model the best quality data on which\\xa0to ground its response, and also the generative part so that the LLM can give the richest, best\\xa0response finally to the user when it generates the answer. Thank you for learning more about RAG\\xa0and like and subscribe to the channel. Thank you.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_transcript_tool_output = tool_mapping[tool_calls_2[0]['name']].invoke(tool_calls_2[0]['args'])\n",
    "fetch_transcript_tool_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "You're adding the transcript content to your conversation history by creating another `ToolMessage` that contains the transcript text and the ID of the tool call that requested it. This gives the LLM access to the actual video content so it can generate a summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(ToolMessage(content = fetch_transcript_tool_output, tool_call_id = tool_calls_2[0]['id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the final summary by sending your complete conversation history to the LLM. Now that the model has access to both the video ID and the full transcript, you'll invoke it one more time to generate the summary that the user requested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The video features Marina Danilevsky, a Senior Research Scientist at IBM Research, discussing a framework called Retrieval-Augmented Generation (RAG) for improving the accuracy and currency of large language models (LLMs).\\n\\n### Summary:\\n\\n1. **Introduction to LLM Challenges**:\\n   - LLMs can generate accurate and in-depth responses but may also produce incorrect or outdated information due to a lack of sourcing and reliance on training data alone.\\n\\n2. **Anecdote Example**:\\n   - Danilevsky shares a personal anecdote about answering her children\\'s question regarding which planet has the most moons. She initially states Jupiter without verifying the information, highlighting issues of confidence without supporting data and outdated knowledge.\\n\\n3. **Importance of Reliable Retrieval**:\\n   - RAG aims to address the challenges faced by LLMs by integrating a retrieval mechanism. Instead of solely relying on pre-existing knowledge, LLMs can query a content store (like the internet or databases) to retrieve the most relevant and current information before generating a response.\\n\\n4. **How RAG Works**:\\n   - User prompts the LLM with a question.\\n   - The LLM first retrieves relevant content from the data store, combines that with the user\\'s question, and then generates a well-supported answer.\\n\\n5. **Benefits of RAG**:\\n   - Addresses the problems of outdated information by updating the content store with new data instead of retraining the LLM.\\n   - Encourages accurate sourcing, reducing the likelihood of hallucinations (making up information) and allowing the model to confidently say \"I don\\'t know\" if an answer cannot be reliably derived.\\n\\n6. **Call to Action**:\\n   - The video concludes with a note on the importance of improving both the retrieval quality and the generative capabilities of LLMs to ensure they provide informative and accurate responses.\\n\\nOverall, the RAG framework enhances the reliability of LLMs, making them more trustworthy and effective in providing responses to user queries.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 400, 'prompt_tokens': 1811, 'total_tokens': 2211, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': None, 'id': 'chatcmpl-Bu1BnPs6RVDqN7IoGPjITqKlWMUJE', 'finish_reason': 'stop', 'logprobs': None}, id='run--13f2c98d-df87-4f12-b79b-154cf5044966-0', usage_metadata={'input_tokens': 1811, 'output_tokens': 400, 'total_tokens': 2211, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating the tool calling process\n",
    "\n",
    "You manually saw how you input a text request to your LLM, where the LLM recognized that a tool call was required. Then, you extracted the tool content, formatted the input, made the next tool call, and repeated these steps. While this step-by-step approach helps understand the process, it would be tedious to implement for every application. Now let's automate this entire workflow.\n",
    "\n",
    "#### Extracting tool information from LLM response\n",
    "Create a function to automate tool calling. The input is the tool call object from which you extract the name, and use the tool_mapping dictionary to find the correct function to call. You'll pass the arguments from the tool call to this function and then send the output back as a ToolMessage with the tool_call_id included.\n",
    "The tool_call_id is an essential part of this process as it links each tool response back to the specific tool request made by the language model. This ID ensures the LLM can match responses to its requests, which is crucial when multiple tools are called in sequence or simultaneously. Without this ID, the LLM would have no way to know which response corresponds to which request, making multi-step reasoning impossible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the processing steps\n",
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute single tool call and return ToolMessage\"\"\"\n",
    "    try:\n",
    "        result = tool_mapping[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        return ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        return ToolMessage(\n",
    "            content=f\"Error: {str(e)}\",\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to chain all your functions or tools together, but before you do so, you need to format the data properly. Not only are you required to store the output of each tool, but you also need to store state information like tool IDs. To do this effectively, you must ensure the output of each tool can be properly passed to the next step in your pipeline. The RunnablePassthrough component allows you to maintain state throughout the chain while adding or transforming data at each step, making it ideal for connecting your various tools into a cohesive workflow.\n",
    "The RunnableLambda, placed at the end of your chain, serves a different purpose - it extracts only the final result you want to present to the user. After all the tool calls and message processing, you have a rich state object with many fields, but the user typically only needs the final answer. The RunnableLambda transforms this complete state into just the information you want to return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the summarization chain\n",
    "\n",
    "Now, you'll combine your functions into a complete `summarization_chain` using the pipe operator `|`, which applies functions sequentially (similar to function composition where `f|g(x)` is equivalent to `f(g(x))`).\n",
    "\n",
    "The workflow follows these steps:\n",
    "1. Convert the input prompt to a HumanMessage\n",
    "2. Pass the message to LLM with tools\n",
    "3. Extract tool calls from LLM response\n",
    "4. Update message history with tool results\n",
    "5. Send updated messages back to LLM\n",
    "6. Repeat steps 3-5 as needed\n",
    "7. Finally, extract just the content from the final message using RunnableLambda\n",
    "\n",
    "Each step maintains state using RunnablePassthrough until you reach the final message, at which point you'll apply RunnableLambda to extract only the summary text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_chain = (\n",
    "    # Start with initial query\n",
    "    RunnablePassthrough.assign(\n",
    "        messages=lambda x: [HumanMessage(content=x[\"query\"])]\n",
    "    )\n",
    "    # First LLM call (extract video ID)\n",
    "    | RunnablePassthrough.assign(\n",
    "        ai_response=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    "    )\n",
    "    # Process first tool call\n",
    "    | RunnablePassthrough.assign(\n",
    "        tool_messages=lambda x: [\n",
    "            execute_tool(tc) for tc in x[\"ai_response\"].tool_calls\n",
    "        ]\n",
    "    )\n",
    "    # Update message history\n",
    "    | RunnablePassthrough.assign(\n",
    "        messages=lambda x: x[\"messages\"] + [x[\"ai_response\"]] + x[\"tool_messages\"]\n",
    "    )\n",
    "    # Second LLM call (fetch transcript)\n",
    "    | RunnablePassthrough.assign(\n",
    "        ai_response2=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    "    )\n",
    "    # Process second tool call\n",
    "    | RunnablePassthrough.assign(\n",
    "        tool_messages2=lambda x: [\n",
    "            execute_tool(tc) for tc in x[\"ai_response2\"].tool_calls\n",
    "        ]\n",
    "    )\n",
    "    # Final message update\n",
    "    | RunnablePassthrough.assign(\n",
    "        messages=lambda x: x[\"messages\"] + [x[\"ai_response2\"]] + x[\"tool_messages2\"]\n",
    "    )\n",
    "    # Generate final summary\n",
    "    | RunnablePassthrough.assign(\n",
    "        summary=lambda x: llm_with_tools.invoke(x[\"messages\"]).content\n",
    "    )\n",
    "    # Return just the summary text\n",
    "    | RunnableLambda(lambda x: x[\"summary\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you invoke the summarization chain with a YouTube video URL; this passes your query containing a YouTube URL to the chain, which automatically extracts the video ID, fetches the transcript, and generates a summary of the content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " The YouTube video features Marina Danilevsky, a Senior Research Scientist at IBM Research, discussing a framework called Retrieval-Augmented Generation (RAG) designed to improve the accuracy and up-to-dateness of large language models (LLMs). \n",
      "\n",
      "Danilevsky explains that LLMs generate texts based on user queries but often provide incorrect or outdated information due to reliance on their training data without verification from reliable sources. She illustrates this issue with a personal anecdote about mistakenly identifying the planet with the most moons. \n",
      "\n",
      "RAG addresses these problems by allowing the LLM to first retrieve relevant, current information from a content store before generating a response. This approach enhances the model's accuracy, enabling it to support its answers with valid sources, thereby reducing the likelihood of \"hallucinations\" (producing incorrect information). It also allows the model to recognize when it doesn't know the answer and improve the quality of retrieved data, which is essential for providing reliable responses. \n",
      "\n",
      "In essence, RAG combines retrieval of up-to-date content with generation, leading to better-informed answers to user questions while minimizing the risks associated with outdated or unverifiable information.\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "result = summarization_chain.invoke({\n",
    "    \"query\": \"Summarize this YouTube video: https://www.youtube.com/watch?v=T-D1OfcDW1M\"\n",
    "})\n",
    "\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Up to this point, you've demonstrated how to manually orchestrate the tool calling process step by step. You first invoked the LLM with the user's query, interpreted its decision to use the `extract_video_id` tool, executed that tool, fed the result back to the LLM, processed its next decision to use the `fetch_transcript` tool, executed that tool, and finally had the LLM generate a summary based on the transcript.\n",
    "\n",
    "Now you'll see how to accomplish the same workflow more efficiently using LangChain's chain functionality, which automates this back-and-forth process of tool selection, execution, and response handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the initial message setup\n",
    "\n",
    "Here you're setting up the first step of your chain that will handle the initial user query. The `RunnablePassthrough.assign` creates a component that takes an input dictionary containing a \"query\" and converts it into a list containing a single `HumanMessage` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_setup = RunnablePassthrough.assign(\n",
    "    messages=lambda x: [HumanMessage(content=x[\"query\"])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the first LLM interaction\n",
    "\n",
    "Here, you'll create the second step of your chain, which handles the first interaction with the language model. This component takes the formatted messages from the previous step, sends them to your tool-equipped LLM, and captures the response in a field called \"ai_response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_llm_call = RunnablePassthrough.assign(\n",
    "    ai_response=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the first tool call\n",
    "\n",
    "Here, you're defining the processing step that handles the LLM's first tool call. This component:\n",
    "1. Executes each tool call by passing it to your `execute_tool` function, which runs the appropriate tool and returns the result as a `ToolMessage`\n",
    "2. Updates the message history by combining the original messages, the LLM's response, with the tool calls, and the tool results\n",
    "3. Prepares the updated conversation state for the next interaction with the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tool_processing = RunnablePassthrough.assign(\n",
    "    tool_messages=lambda x: [\n",
    "        execute_tool(tc) for tc in x[\"ai_response\"].tool_calls\n",
    "    ]\n",
    ").assign(\n",
    "    messages=lambda x: x[\"messages\"] + [x[\"ai_response\"]] + x[\"tool_messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the second LLM interaction\n",
    "\n",
    "Here, you're creating the next step in your chain that handles the second interaction with the language model. This component takes the updated message history (which now includes the results from the first tool call) and sends it to the LLM again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_llm_call = RunnablePassthrough.assign(\n",
    "    ai_response2=lambda x: llm_with_tools.invoke(x[\"messages\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the second tool call\n",
    "\n",
    "Here, you're defining the processing step that handles the LLM's second tool call. Similar to the first tool processing step, this component executes the tool calls (typically fetching the transcript), creates tool messages with the results, and updates the message history by combining everything for the final summarization step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_tool_processing = RunnablePassthrough.assign(\n",
    "    tool_messages2=lambda x: [\n",
    "        execute_tool(tc) for tc in x[\"ai_response2\"].tool_calls\n",
    "    ]\n",
    ").assign(\n",
    "    messages=lambda x: x[\"messages\"] + [x[\"ai_response2\"]] + x[\"tool_messages2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the final summary\n",
    "\n",
    "Here, you're defining the final step that produces the summary of the YouTube video. This component:\n",
    "1. Takes the complete message history (which now contains the original query, tool calls, and tool results)\n",
    "2. Invokes the LLM one last time to generate a summary\n",
    "3. Extracts just the content field from the LLM's response\n",
    "4. Uses a RunnableLambda to return only the summary text as the final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = RunnablePassthrough.assign(\n",
    "    summary=lambda x: llm_with_tools.invoke(x[\"messages\"]).content\n",
    ") | RunnableLambda(lambda x: x[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assembling the complete chain\n",
    "\n",
    "Now, you're combining all the individual components you've defined into a single cohesive chain. By piping each step to the next, you'll create a workflow that:\n",
    "1. Formats the initial query\n",
    "2. Gets the first LLM response (video ID extraction)\n",
    "3. Processes the first tool call\n",
    "4. Gets the second LLM response (transcript request)\n",
    "5. Processes the second tool call\n",
    "6. Generates the final summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    initial_setup\n",
    "    | first_llm_call\n",
    "    | first_tool_processing\n",
    "    | second_llm_call\n",
    "    | second_tool_processing\n",
    "    | final_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you're testing your automated chain with the original video summarization query you handled manually before. By passing in the same query to your chain, you can confirm that it produces the same results but in a much more streamlined manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " The video features Marina Danilevsky, a Senior Research Scientist at IBM, discussing the concept of Retrieval-Augmented Generation (RAG) in the context of large language models (LLMs). Here’s a summary of the key points:\n",
      "\n",
      "1. **Understanding LLMs**: While large language models can generate text confidently, they often struggle with providing accurate and up-to-date information. The speaker illustrates this with an anecdote about answering a question about the planet with the most moons, where she mistakenly cites Jupiter due to outdated knowledge.\n",
      "\n",
      "2. **Main Problems**: Two common problems with LLMs are highlighted: the lack of reliable sources for the information they provide and the potential for them to be out-of-date.\n",
      "\n",
      "3. **RAG Framework Introduction**: RAG enhances LLMs by allowing them to retrieve relevant information from a content store (like the internet or specific databases) before generating a response. This ensures that answers are more accurate and grounded in credible sources.\n",
      "\n",
      "4. **Process Improvement**: The RAG framework modifies the response process by incorporating three components: the user’s question, instructions to retrieve relevant content, and the retrieved information itself. This combination helps to provide evidence for the answers generated by the LLM.\n",
      "\n",
      "5. **Benefits of RAG**:\n",
      "   - **Up-to-Date Information**: Instead of retraining the entire model with new data, updating the content store allows the model to provide the latest information easily.\n",
      "   - **Source Credibility**: By referencing primary sources before generating responses, LLMs can reduce the chances of providing fabricated or misleading information, and can also learn to say \"I don't know\" when they cannot provide a reliable answer.\n",
      "\n",
      "6. **Challenges**: There’s a caveat that if the retriever does not have access to high-quality data, the model may still fail to provide accurate answers.\n",
      "\n",
      "The video concludes with an encouragement to explore further learning about the RAG framework, highlighting ongoing efforts in both improving the retrieval process and the generative capabilities of LLMs.\n"
     ]
    }
   ],
   "source": [
    "query = {\"query\": \"I want to summarize youtube video: https://www.youtube.com/watch?v=T-D1OfcDW1M in english\"}\n",
    "result = chain.invoke(query)\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Chain with a Different Query\n",
    "\n",
    "Here, you're testing your completed chain with a new query to demonstrate its flexibility. Instead of requesting a video summary, you're asking for information about trending videos in India. You'll create a dictionary with the query and invoke your chain, which will handle all the necessary tool calls automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Summary:\n",
      " Here are the top 3 trending YouTube videos in India along with their metadata:\n",
      "\n",
      "1. **Title**: [2025 MLB All-Star Game Full Game Highlights (7/15/25) | MLB Highlights](https://www.youtube.com/watch?v=Ykqhonhqios)\n",
      "   - **Channel**: MLB\n",
      "   - **Duration**: 20 minutes and 58 seconds\n",
      "   - **View Count**: 1,038,855 views\n",
      "  \n",
      "2. **Title**: [All The Ghosts You Will Be](https://www.youtube.com/watch?v=xHd4zsIbXJ0)\n",
      "   - **Channel**: Vsauce\n",
      "   - **Duration**: 22 minutes and 0 seconds\n",
      "   - **View Count**: 1,678,547 views\n",
      "  \n",
      "3. **Title**: [Jessica Sanchez Receives A GOLDEN BUZZER From Sofia Vergara | AGT 2025](https://www.youtube.com/watch?v=a-V1XTvsPq8)\n",
      "   - **Channel**: America's Got Talent\n",
      "   - **Duration**: 8 minutes and 40 seconds\n",
      "   - **View Count**: 2,374,005 views\n",
      "\n",
      "Feel free to ask if you need additional information!\n"
     ]
    }
   ],
   "source": [
    "query = {\"query\": \"Get top 3 youtube videos in India and their metadata\"}\n",
    "result = summarization_chain.invoke(query)\n",
    "print(\"Video Summary:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive chain flow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've created a chain that works well for your specific two-step tool calling process, you need to consider more complex scenarios. Your current chain is limited to exactly two tool calls in a fixed sequence. In real-world applications, you might need a variable number of tool calls depending on the user's query - for example, getting trending videos and then fetching metadata for each video, or searching for videos on a topic and then getting transcripts for multiple results.\n",
    "\n",
    "To handle these more complex scenarios, you'll build a recursive chain that can dynamically decide how many tool calls are needed and continue processing until all necessary information has been gathered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "import json\n",
    "\n",
    "def execute_tool(tool_call):\n",
    "    \"\"\"Execute single tool call and return ToolMessage\"\"\"\n",
    "    try:\n",
    "        result = tool_mapping[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "        content = json.dumps(result) if isinstance(result, (dict, list)) else str(result)\n",
    "    except Exception as e:\n",
    "        content = f\"Error: {str(e)}\"\n",
    "    \n",
    "    return ToolMessage(\n",
    "        content=content,\n",
    "        tool_call_id=tool_call[\"id\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the core processing logic\n",
    "\n",
    "This function handles the core processing logic of your recursive chain. It takes the current conversation history and:\n",
    "\n",
    "1. Identifies the most recent message in the conversation\n",
    "2. Extracts all tool calls from that message and executes them in parallel using your `execute_tool` helper\n",
    "3. Updates the message history by adding the tool response messages\n",
    "4. Gets the next response from the language model based on the updated conversation\n",
    "5. Returns the complete updated message history with both tool responses and the new LLM response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tool_calls(messages):\n",
    "    \"\"\"Recursive tool call processor\"\"\"\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Execute all tool calls in parallel\n",
    "    tool_messages = [\n",
    "        execute_tool(tc) \n",
    "        for tc in getattr(last_message, 'tool_calls', [])\n",
    "    ]\n",
    "    \n",
    "    # Add tool responses to message history\n",
    "    updated_messages = messages + tool_messages\n",
    "    \n",
    "    # Get next LLM response\n",
    "    next_ai_response = llm_with_tools.invoke(updated_messages)\n",
    "    \n",
    "    return updated_messages + [next_ai_response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the recursive stopping condition\n",
    "\n",
    "This function determines whether your recursive process should continue or terminate. It:\n",
    "\n",
    "1. Takes the current message history and examines the last message\n",
    "2. Checks if that message contains any tool calls using the `getattr` function (which safely handles cases where the attribute might not exist)\n",
    "3. Returns a boolean value - `True` if there are more tool calls to process, and `False` when you reach a point where the LLM has provided a final answer without requesting additional tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(messages):\n",
    "    \"\"\"Check if you need another iteration\"\"\"\n",
    "    last_message = messages[-1]\n",
    "    return bool(getattr(last_message, 'tool_calls', None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Implementing the recursive function\n",
    "\n",
    "This function implements the actual recursion that powers your dynamic tool calling process:\n",
    "\n",
    "1. It first checks the stopping condition using the `should_continue` function to determine if more tools need to be called\n",
    "2. If more tool calls are needed, it processes those calls using your `process_tool_calls` function and then recursively calls itself with the updated messages\n",
    "3. If no more tool calls are needed, it returns the final message history, which contains the complete conversation, including the LLM's final response\n",
    "\n",
    "After defining this recursive function, you'll wrap it in a `RunnableLambda` to make it compatible with LangChain's chain architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recursive_chain(messages):\n",
    "    \"\"\"Recursively process tool calls until completion\"\"\"\n",
    "    if should_continue(messages):\n",
    "        new_messages = process_tool_calls(messages)\n",
    "        return _recursive_chain(new_messages)\n",
    "    return messages\n",
    "\n",
    "recursive_chain = RunnableLambda(_recursive_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the complete universal chain\n",
    "\n",
    "Now, you're assembling your final universal chain that can handle any type of query requiring any number of tool calls. This chain consists of three main steps:\n",
    "\n",
    "1. The first step converts the user query into a properly formatted `HumanMessage` object\n",
    "2. The second step sends this initial message to your tool-equipped LLM and adds the LLM's first response to the message history\n",
    "3. The final step passes the conversation to your recursive chain, which will handle all subsequent tool calls until the LLM provides a final answer\n",
    "\n",
    "This universal chain is much more flexible than your earlier fixed-step chain, as it can dynamically adapt to queries that require different numbers and types of tool calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_chain = (\n",
    "    RunnableLambda(lambda x: [HumanMessage(content=x[\"query\"])])\n",
    "    | RunnableLambda(lambda messages: messages + [llm_with_tools.invoke(messages)])\n",
    "    | recursive_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Here are the top 3 trending videos in the US along with their metadata and thumbnails:\\n\\n### 1. **2025 MLB All-Star Game Full Game Highlights (7/15/25) | MLB Highlights**\\n- **Channel:** MLB\\n- **Views:** 1,081,331\\n- **Duration:** 20:58 minutes\\n- **Likes:** 18,484\\n- **Comments:** 2,000\\n- **[Watch Video](https://www.youtube.com/watch?v=Ykqhonhqios)**\\n\\n**Thumbnails:**\\n- ![Thumbnail 1](https://i.ytimg.com/vi/Ykqhonhqios/maxresdefault.jpg)\\n  \\n### 2. **All The Ghosts You Will Be**\\n- **Channel:** Vsauce\\n- **Views:** 1,729,055\\n- **Duration:** 22:00 minutes\\n- **Likes:** 255,327\\n- **Comments:** 29,000\\n- **[Watch Video](https://www.youtube.com/watch?v=xHd4zsIbXJ0)**\\n\\n**Thumbnails:**\\n- ![Thumbnail 1](https://i.ytimg.com/vi/xHd4zsIbXJ0/maxresdefault.jpg)\\n\\n### 3. **Jessica Sanchez Receives A GOLDEN BUZZER From Sofia Vergara | AGT 2025**\\n- **Channel:** America's Got Talent\\n- **Views:** 2,414,537\\n- **Duration:** 8:39 minutes\\n- **Likes:** 89,822\\n- **Comments:** 10,000\\n- **[Watch Video](https://www.youtube.com/watch?v=a-V1XTvsPq8)**\\n\\n**Thumbnails:**\\n- ![Thumbnail 1](https://i.ytimg.com/vi/a-V1XTvsPq8/maxresdefault.jpg)\\n\\nFeel free to click on the video titles to watch them!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 402, 'prompt_tokens': 8677, 'total_tokens': 9079, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': None, 'id': 'chatcmpl-Bu2urIXtk0sVSO9kv0syHnXNJud5T', 'finish_reason': 'stop', 'logprobs': None} id='run--dc7260df-26fb-4260-9d39-0c53d0387c26-0' usage_metadata={'input_tokens': 8677, 'output_tokens': 402, 'total_tokens': 9079, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(universal_chain.invoke({\n",
    "    \"query\": \"Show top 3 US trending videos with metadata and thumbnails\"\n",
    "})[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Try a different video with a Youtube link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "video_link = \"https://www.youtube.com/watch?v=YdqKZDJQoDw&ab_channel=FoxNews\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_id = \"INSERT_VIDEO_ID_HERE\"  # Replace with the actual video ID\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Extract the video ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YdqKZDJQoDw\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "video_id = extract_video_id.run(\"https://www.youtube.com/watch?v=YdqKZDJQoDw&ab_channel=FoxNews\")\n",
    "print(video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_id = extract_video_id.run(youtube_url)\n",
    "print(f\"Extracted video ID: {video_id}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Collect all necessary data about the video in one go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "channel": "Fox News",
       "chapters": null,
       "comments": 3600,
       "duration": 939,
       "likes": 17442,
       "title": "‘LIKE BUTTER’: Trump dishes on US strikes against Iran",
       "views": 1140229
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "meta_data=get_full_metadata.run(\"https://www.youtube.com/watch?v=YdqKZDJQoDw&ab_channel=FoxNews\")\n",
    "display(JSON(meta_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "video_metadata = get_full_metadata.run(youtube_url)\n",
    "print(f\"Retrieved metadata for: {video_metadata['title']}\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Get video transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ACROSS THE, FINISH LINE. MR. PRESIDENT, YOU WERE JUST COMING OFF ONE OF THE MOST CONSEQUENTIAL WEEKS ANYBODY HAS EVER SEEN FROM STRIKING IRAN, GOING TO NATO, ANNOUNCING TWO DEALS AND THEN THIS SUPREME COURT VICTORY. HAVE YOU BEEN ABLE TO DIGEST WHAT HAS TO OCCURRED IN THE LAST SEVEN DAYS? >> NOT REALLY. IT WAS PRETTY WILD PERIOD OF TIME. I REALLY THINK IN SIX MONTHS WE'VE TAKEN THE COUNTRY AND TURNED IT AROUND. IT'S -- I WAS TOLD BY THE KING OF SAUDI ARABIA, BY THE LEADER OF QATAR AND THE LEADER OF THE UAE WHEN WE WENT OVER THERE AND WE BROUGHT BACK $5.1 TRILLION INVESTMENT INTO THE UNITED STATES, TODAY SAID, YOU KNOW, YOU'RE PRESIDING OVER THE HOTTEST COUNTRY IN THE WORLD. A YEAR AGO WE THOUGHT YOUR COUNTRY WAS DEAD, IT COULD NEVER COME BACK. IT WAS SO INCOMPETENTLY RUN BY A VERY BAD PRESIDENT. AND I SAID, YOU'RE RIGHT, THIS COUNTRY IS HOT. AND THAT WAS A COUPLE OF MONTHS AGO. YOU KNOW, NOW WE'RE TALKING ABOUT WHAT WE'VE DONE RECENTLY. SO THE COUNTRY IS REALLY GOING, AND THE NUMBERS ARE GREAT. I WATCH YOU IN THE MORNING, AND YOU'RE GIVING THOSE NUMBERS, AND CAN THEY'RE REALLY GOOD. NO INFLATION. WE HAVE A BAD FED CHAIRMAN, BUT OTHER THAN THAT WE HAVE, YOU KNOW, GREAT. IT DOESN'T EVEN MATTER, THE NUMBERS ARE SO GOOD THAT IT DOESN'T MATTER HE KEEPS THE RATES ARTIFICIALLY HIGH. BUT IT'S BEEN AN AMAZING PERIOD OF TIME. MARIA: WENT YOU WENT TO THE MIDDLE EAST AND YOU STRENGTHENED THOSE RELATIONSHIPS WITH ALL OF THOSE COUNTRIES YOU JUST MENTIONED, DID YOU HAVE AN IDEA THAT YOU WOULD HAVE TO DO SOMETHING WITH REGARD TO IRAN? BECAUSE YOU TEASED IT A LITTLE. YOU SAID, LOOK, THEY'VE GOT TO COME TO THE TABLE AND TALK, SOMETHING'S GOING TO HAPPEN. >> WELL, IT WAS GETTING A LITTLE BIT HOT, AND WE KNEW THEY HAD A LOT OF SITES, PROBABLY THREE PLUS THE ONE. BUT IT'S -- THEY HAD THREE MAIN SITES, AND WE KNEW THEY WERE GOING TO HAVE THE EITHER GIVE THEM UP -- AND I THOUGHT WE COULD DO IT DURING NEGOTIATION, AND WE JUST ABOUT HAD IT DONE, AND THEN THEY SAID WE WANT ENRICHMENT. ENRICHMENT DOESN'T MEAN AIR-CONDITIONING OR TO JACK UP YOUR CAR. ENRICH ENRICHMENT IS A BAD WORD WITH. I SAID, YOU'VE GOT SO MUCH OIL, WHAT DO YOU NEED IT FOR? TODAY SAID IS, WELL, WE NEED IT, WE NEED IT. I WOULDN'T ALLOW THAT TO HAPPEN. I THINK PEOPLE WOULDN'T HAVE UNDERSTOOD IF IT I HAD ALLOWED THAT TO HAPPEN. WE HAD A 60-DAY TALK AND THAT DELAYED THEM A LOT, AND I SAID, LET'S GO AT IT. AND IT JUST WORKED OUT. WE WANT IT TO WORK OUT ALSO FOR IRAN. THEY WERE BEATEN UP. AND SO WAS ISRAEL, IN ALL FAIRNESS. WE CALL IT THE 12-DAY WAR. THAT WAS AN INTENSIVE WAR. BUT THE THING THAT I WANTED TO DO AND I'VE SAID IT ON YOUR SHOW IF YOU LOOK BACK 30 YEARS AGO OR 25 YEARS, WE'VE BEEN DOING YOUR SHOW A LONG TIME. NOBODY LIKE YOU. BUT YOU LOOK BACK AT THE EARLY INTERVIEW ISES, I WOULD SAY IRAN CANNOT HAVE A NUCLEAR WEAPON. THEY'VE WANTED THIS FOR YEARS, AND THEY WERE WEEKS AWAY FROM GETTING IT. AND THOSE PILOTS WENT IN THERE WITH THAT -- THOSE BEAUTIFUL PLANES AND THE MOST SOPHISTICATED BOMBS ANYWHERE IN THE -- THINK OF IT. THEY CAN GO 30 STORIES DEEP INTO GRANITE. WHO WOULD THINK THAT'S EVEN POSSIBLE? AND THEY DID OBLITERATE IT, IT TURNED OUT. THEN WE HAD THE FAKE NEWS WHERE CNN AND THE NEW YORK TIMES WERE SAYING, WELL, MAYBE IT WASN'T AS GOOD AS TRUMP SAID. MAYBE IT WASN'T TOTALLY TO BLITZ RATED. IT WAS DESTROYED BUT NOT -- YOU KNOW, JUST HORRIBLE. I COULD SEE IT HAPPENING, AND THEY TRIED TO BUILD THAT INTO A STORY, AND THEN IT TURNED OUT, NO, IT WAS OBLITERATED LIKE NOBODY'S EVER SEEN BEFORE. AND THAT MEANT THE END TO THEIR NUCLEAR AMBITIONS AT LEAST FOR A PERIOD OF TIME. AND, YOU KNOW, A LOT OF PEOPLE HAVE SAID, WELL, DO YOU THINK THEY'RE GOING TO START AGAIN? I SAID, THE LAST THING THEY WANT TO DO RIGHT NOW IS THINK ABOUT NUCLEAR. THEY HAVE TO PUT THEMSELVES BACK INTO CONDITION, IN SHAPE. MARIA: YOU TWEETED, THE DEMOCRATS LEAKED AN INTELLIGENC- >> THEY SHOULD BE PROSECUTED. MARIA: SPECIFICALLY -- [INAUDIBLE CONVERSATIONS] >> THEY COULD FIND OUT IF THEY WANTED. THEY COULD FIND OUT EASILY. TELL THE REPORTER, NATIONAL SECURITY. WHO GAVE IT? YOU HAVE TO DO THAT THAT. I SUSPECT WE'LL BE DOING THINGS LIKE THAT. MARIA: WELL, THOSE PILOTS WERE SO COURAGEOUS, AND, YOU KNOW, YOU WANT TO HONOR THEM. ARE YOU GOING TO DO SOMETHING FOR THEM? >> YES, THEY'RE GOING TO COME TO TO THE WHITE HOUSE. BUT WHAT YOU SAID IS RIGHT. THESE PEOPLE FLEW 36 HOURS IN A SMALL SPACE, A BIG PLANE BUT A SMALL SPACE. IT WAS MOSTLY OCCUPIED BY BOMBS. AND THEY FLEW SO BRILLIANTLY, AND THEY HIT A TARGET THE SIZE OF THIS CIRCLE. A LITTLE TARGET. THEY SAY HALF THE SIZE OF A REFRIGERATOR DOOR FROM 50,000 FEET UP IN THE AIR GOING AT A RAPID SPEED BECAUSE THEY'RE GOING VERY FAST WHEN, YOU KNOW, THEY'RE OVER A PRETTY ROUGH TERRITORY. AND THEY HID IT -- HIT IT EVERY SINGLE TIME. AND THEN THEY KNOCKED OUT TWO OTHER SITES ASIDE FROM THAT. MARIA: DO YOU THINK THAT THE IRANIAN REGIME HID SOME OF THE ENRICHED URANIUM BEFORE THE STRIKES? >> NO. YOU MEAN DID THEY TAKE IT OUT OF THE DEEP ONE. >> YEAH. >> NO. MARIA: BECAUSE YOU WERE PRETTY CLEAR YOU CAN'T HAVE A WEAPON, YOU'RE GOING TO HAVE TO COME TO THE TALK, SOMETHING OOH'S GOING TO HAPPEN. >> NO, I DON'T -- FIRST OF ALL, IT'S VERY HARD TO DO, IT'S VERY DANGEROUS TO DO. IT'S VERY HEAVY, IT'S A VERY, VERY HARD THING TO DO. PLUS, WE DIDN'T GIVE THEM MUCH NOTICE BECAUSE THEY DIDN'T KNOW WE WERE COMING, AND NOBODY THOUGHT WE'D GO AFTER THAT SITE BECAUSE EVERYBODY SAID THAT THAT SITE WAS IMPENETRABLE. IT'S AT THE BOTTOM OF A MOUNTAIN, AND IT'S GRANITE. YOU KNOW, GRANITE'S THE HARDEST STONE, AND IT'S GRANITE. BUT I DO BELIEVE THEY HAD PEOPLE THERE BECAUSE THERE WERE CARS. THOSE WERE MASONS, PEOPLE WORKING WITH CONCRETE. THEY WERE TRYING TO SEAL UP THE ENTRANCE TO WHERE THE BOMB WOULD MOST LIKELY GO IN, AND THEY DID THAT. THEY WERE WORKING ON THAT, AND THE BOMB WENT THROUGH IT LIKE IT WAS BUTTER, LIKE IT WAS ABSOLUTE BUTTER. MARIA: BECAUSE I SAW REPORTS THAT THAT THERE WERE 400 KILOGRAMS, 800 POUNDS THAT THEY MOVED, BUT I WONDER IF IT'S TRACE,,. I MEAN, IF THEY WERE TO -- >> THEY DIDN'T MOVE ANYTHING. MARIA: THEY DIDN'T MOVE ANYTHING? >> YOU KNOW WHAT THEY MOVED? THEMSELVES. THEY WERE ALL TRYING TO LIVE. THEY DIDN'T THINK IT WAS GOING TO BE ACTUALLY DOABLE, WHAT WE DID. WHAT WE DID WAS AMAZING. THERE WERE ENERGY COMMISSIONS THAT WENT -- IT'S JUST THOUSANDS OF TONS OF ROCK IN THAT THAT ROOM RIGHT NOW. THAT ROOM, THE WHOLE PLACE WAS JUST DESTROYED. AND THE OTHER TO -- TWO ALSO. NOW, ISRAEL WAS ABLE TO DO DAMAGE, BUT WE DID THE FINAL DAMAGE. AND WE HAVE THE GREATEST SUBMARINES IN THE WORLD. WE LAUNCHED 30 ROCKETS FROM SUBMARINES. EVERY SINGLE ONE OF THEM HIT THEIR TARGET. MORE. MARIA: MORE 40 WILL YOU KNOW IF -- HOW WILL YOU KNOW IF HE TRIES TO START THINGS UP AGAIN? BECAUSE IRAN STOPPED COOPERATION WITH THE U.N. NUCLEAR WATCHDOG. >> THE LAST THING THEY'RE GOING TO BE DOING RIGHT NOW FOR A PERIOD OF TIME AT LEAST IS NUCLEAR. THEY'VE HAD IT. THEY'VE BEEN TRYING IT FOR 25 YEARS. THE LAST THING THEY'RE GOING TO DO -- WE HAD TO HIT THEM THOUGH. THEY WERE CHOSE TO GETTING A NUCLEAR BOMB, ABSOLUTELY. MARIA: YEAH. I'VE HEARD FROM PEOPLE INSIDE IRAN THAT THE REGIME IN IRAN HAS EXECUTED MORE IRANIAN CIVILIANS IN THE PAST 48 HOURS THAN THE NUMBER OF IRANIANS WHO DIED IN THE 12-DAY WAR. SO YOU'VE GOT A REGIME THAT'S BEEN BUILDING BALLISTIC MISSILES, BUILDING OR TRYING TO BUILD A NUCLEAR WEAPON. WHY SHOULD WE THINK THAT THEY'RE ACTUALLY GOING TO GIVE UP ON IN THIS? >> WELL, THEY MIGHT NOT GIVE UP, BUT THEY'RE GOING TO GIVE UP ON -- THEY'RE EXHAUSTED. THEY ARE EXHAUSTED. THEY TOOK HITS LIKE NOBODY'S EVER TAKEN. AND, FRANKLY, ISRAEL -- IT WAS THE PERFECT TIME. WE WENT IN, WE DESTROYED THEIR NUCLEAR CAPABILITY, AND WE STOPPED. IT WAS A BEAUTIFUL THING. AND THEY COULDN'T HAVE DEVELOP ON MUCH LONGER. AND ISRAEL -- THAT WAS A VERY INTENSE 12 DAYS, VERY, VERY INTENSE. MARIA: HAS ANY OTHER COUNTRY SUGGESTED TO YOU RECENTLY AS A RESULT OF IN THAT THEY WANT TO JOIN THE ABRAHAM ACCORDS? >> YES. SO WE HAVE SOME REALLY GREAT COUNTRIES IN THERE RIGHT NOW, AND I THINK WE'RE GOING TO START LOADING THEM UP BECAUSE IRAN WAS THE PRIMARY PROBLEM. I ACTUALLY THOUGHT IRAN -- I ACTUALLY THOUGHT WE HAD A PERIOD OF TIME WHERE I THOUGHT IRAN WOULD JOIN THE ABE AM ACCORDS ALONG WITH EVERYBODY ELSE AND, FRANKLY, THEY WOULD HAVE BEEN BETTER OFF THAN WHERE THEY ARE RIGHT NOW. MARIA: AND WHAT ABOUT THE ABRAHAM ACCORDS? WOULD SYRIA ENTER? >> WELL, I DON'T KNOW, BUT I DID TAKE OFF THE SANCTIONS AT THE REQUEST OF SOME OF THE COUNTRIES IN THE AREA THAT ARE FRIENDS OF OURS. I TOOK OFF THE SANCTIONS ON SYRIA TO GIVE THEM A CHANCE AT -- YOU KNOW, THE SANCTIONS ARE BITING, THEY'RE VERY STRONG. AND WE HAVE SANCTIONS ON IRAN TOO. AND, YOU KNOW, YOU TAKE THEM OFF WHEN -- I SORT OF GAVE THE EXPRESSION TODAY, YOU GET MORE SOMETIMES WITH HONEY THAN YOU DO WITH VINEGAR. MARIA: SO YOU WAIVED THE SANCTIONS ON OIL WITH REGARD TO CHINA. SO YOU'RE ALLOWING CHINA TO STILL BUY THE OIL FROM IRAN -- >> NO, I DIDN'T SAY THAT. WE HAVE THE SANCTIONS ON, AND IF THEY DO A JOB AND THEY CAN THE BE PEACEFUL AND THEY CAN SHOW US THEY'RE NOT GOING TO DO ANY MORE HARM, I WOULD TAKE THE SANCTIONS OFF. AND THE SANCTIONS WOULD MAKE A BIG DIFFERENCE, AS YOU KNOW. YOU KNOW, WHEN I WAS -- WE HAD A GREAT SUCCESS IN THE FIRST TERM. WE HAD THE GREATEST ECONOMY IN THE HISTORY OF OUR COUNTRY. BY THE WAY, IT'S GOING TO BE ABSOLUTELY -- THIS IS BE. THIS IS GOING TO BE, I THINK, EVEN MUCH BETTER. WE HAD THE GREATEST ECONOMY IN HISTORY, AND WE USE SANCTIONS. YOU USE THEM CAREFULLY. SANCTIONS COST US A LOT OF MONEY. T NOT JUST, GEEAR WINING -- WE'RE GOING TO PUT SANCTIONS ON. ONE OF THE THINGS I WAS THINKING ABOUT DOING, YOU START WAIFING THEM FOR -- WAIVING THEM FOR COUNTRIES LIKE, IF THEY BEHAVE THEMSELF THEMSELVES, LIKE IRAN WHERE THEY CAN SELL OIL AND DO THE THINGS YOU WANT TO BE ABLE TO DO. THE SANCTIONS ARE VERY POWERFUL, AND IT MAKES IT DIFFICULT. I'M NOT LOOKING TO MAKE IT DIFFICULT CAN. I WANT THEM TO HAVE A GOOD LIFE. BUT THEY WERE TRYING TO DEVELOP A BOMB, AND THE REASON YOU TRY AND DEVELOP A BOMB LIKE THAT SO TO USE IT. MARIA: BUT I WANT TO GET YOUR TAKE ON CHINA BECAUSE YOU'VE ANNOUNCED AN AGREEMENT WITH THE CHINESE COMMUNIST PARTY WHERE CHINA WILL EXPORT THE RARE EARTH MINERALS. WHAT DOES THAT MEAN IN TERMS OF THIS UPCOMING TARIFF STRUCTURE? YOU'VE GOT A PAUSE IN TERMS OF THE MORE STEEP TARIFFS, THAT ENDS AUGUST 12TH. ARE YOU GOING TO EXTEND THAT? ARE YOU GOING TO KEEP THAT IN PLACE OR GO BACK TO A HIGHER TARIFF? >> CHINA'S GOING TO BE PAYING A LOT OF TARIFFS, BUT WE HAVE A BIG DEFICIT. THEY UNDERSTAND THAT. I HAVE A GREAT RELATIONSHIP WITH PRESIDENT XI. WE HAVE TO DOING MANAGER ABOUT THE DEFICIT -- SOMETHING ABOUT THE DEFICIT, TRADE DEFICIT. BIDEN LET THEM JUST TAKE THEM OVER THE COALS. WE'RE NOT GOING TO DO THAT. BUT WE'RE DOING A RARE EARTH DEAL, BUT WE'RE ALSO DOING -- REMEMBER, WE HAVE ALL OF THE AIRPLANE PARTS. THEY CAN'T FLY THEIR AIRPLANES WITHOUT US. IT SHOULD HAVE NEVER GOTTEN THAT WAY BECAUSE WE SHOULD HAVE BEEN DOING -- THEY CALL IT MAGNETS. IT'S RARE EARTH, BUT THEY CALL IT MAGNETS, A SPECIFIC TYPE. AND THEY'RE MAKING THOSE MAGNETS FOR US RIGHT NOW, AND WE HAVE A DEAL WITH CHINA. IT'S A GOOD DEAL I THINK, HOPEFULLY, FOR EVERYBODY. WE HAD A DEAL WITH CHINA WHERE IT WAS 145% TARIFFS, AND I WILL TELL YOU EVERYTHING STOPPED IN CHINA. I SAW THAT. EVERYTHING STOPPED. FINISH -- AND WE DID CHINA A FAVOR. BUT WE'RE GETTING ALONG WELL WITH CHINA. I THINK GETTING ALONG WELL WITH CHINA IS A VERY GOOD THING, BUT THEY ARE PAYING SUBSTANTIAL TARIFFS. MARIA: WELL, I NOTICED THAT THAT. IT SEEMS LIKE YOU GO SO FAR WITH CHINA, BUT YOU DON'T SORT OF USE THE LEVERAGE THAT YOU CAN USE -- >> IF I EVER HAD TO USE IT, I'D USE IT. BUT WHEN THERE'S NO REASON TO USE IT, THAT'S GOOD TOO. MARIA: WE DID JUST ARREST THREE OR FOUR CHINESE NATIONALS IN THAT TRIED TO BRING A PATHOGEN INTO THE COUNTRY FINISH. >> WE DON'T KNOW WHERE THAT CAME FROM. DID THAT COME FROM THE COUNTRY OR THREE WHACKOS THAT HAPPENED TO CARRY SOMETHING, YOU JUST DON'T -- MARIA: WELL, THERE WAS ONE THAT HE SIGNED, ONE OF THEM SIGNED A PAPER SAYING THAT HE WOULD VALUE MAO MAO SAY DONG'S VALUES SYSTEM. THEN THEY HACK INTO OUR TELECOM SYSTEM, THEY'VE BEEN STEALING INTELLECTUAL PROPERTY, FENTANYL, COVID, ALL OF THIS STUFF. SO HOW DO YOU NEGOTIATE WITH OBVIOUSLY A BAD ACTOR -- >> YOU DON'T THINK WE DO THAT TO THEM? YOU DON'T THINK WE DO THAT TO THEM? WE DO,. WE DO A LOT OF THINGS -- MARIA: SO THAT'S THE WAY THE WORLD WORKS? >> THAT'S THE WAY THE WORLD WORKS. IT'S A NASTY WORLD. MARIA: AND THEN YOU DO A TRADE DEAL? >> WE MADE A LOT OF MONEY WITH THE TRADE DEAL. I DO WHAT WORKS. MARIA: YOU THINK THAT CHINA'S GOING TO STOP BULLYING COMPANIES TO GIVE THEM INFORMATION IN TERMS OF -- IN EXCHANGE FOR GETTING THOSE RARE EARTH MINERALS NOW IN. >> WELL, THE COMPANY SHOULDN'T PUT THEMSELVES IN THAT POSITION, YOU KNOW? THEY REALLY SHOULDN'T. I WILL SAY THIS, CHINA NEEDS THOSE COMPANIES NOW MUCH MORE THAN THEY NEEDED THEM TWO YEAR AGO. TWO YEARS AGO CHINA COULD DO THAT. THEY CAN'T. THEY NEED APPLE. APPLE'S BUILDING, SPENDING $600 BILLION, $700 BILLION IN THE UNITED STATES BUILDING PLANTS. THEY WERE NEVER GOING TO DO THAT. THINK OF IT, $700 BILLION. AWE LO -- A LOT OF THE COMPANIES IN CHINA ARE NOW RELOCATING BACK TO THE UNITED STATES. CHINA'S GOING TO TREAT THEM MUCH BETTER AS THEY NEED THEM. THEY DIDN'T NEED THEM URN BIDEN BECAUSE THEY WERE RIPPING US OFF LEFT AND RIGHT. THEY NEED THEM UNDER TRUMP. MARIA: YOU JUST HAD A BIG SUCCESS AT NATO, GETTING THE OTHER MEMBERS -- IS THERE NEGATIVE YOU CAN DO TO GET CONGRESS TO CODIFY IT, TO GET THEIR GOVERNMENTS TO CODIFY IT? >> WHAT YOU DO, THE BEST INSURANCE IS IF YOU DON'T DO IT, YOU LEAVE. YOU DON'T DO BUSINESS WITH THEM ANYMORE, YOU KNOW? YOU DON'T DO BUSINESS. YOU HAVE TO HAVE A CERTAIN TRUST, BUT YOU CAN SAY TRUST BUT VERIFY. WE'RE MAKING UNBELIEVABLE -- MARIA: THERE IS LITIGATION AROUND THE LEGALITY OF THOSE TARIFFS. ARE YOU GOING TO BE ABLE TO GET MORE TRADE DEALS DONE BEFORE YOU HEAR FROM THE INTERNATIONAL COURT OF APPEALS IN AUGUST? >> WE WON THE LITIGATION, AND WE HAD THE DELAY AND WE HAD SOME RADICAL-LEFT JUDGE, AND THEN WE HAD IT OVERTURNED. WE'RE DOING WELL IN ALL OF OUR LITIGATION. WE'RE DOING WELL ON THE TARIFF LITIGATION. LOOK, IF IS SOME JUDGE SAID WE CAN'T DO TARIFFS, WE WOULD FALL PREY TO THE REST OF THE WORLD WHO WOULD DO TARIFFS AND ARE DOING TARIFFS ON US. IF WE COULDN'T FIGHT THEM BACK WITH TARIFFS, THIS COUNTRY WOULD BE IN TERRIBLE SHAPE. WE WOULD BE, LIKE, THIS INNOCENT LAMB BEING SLED TO THOUGHT -- LED TO SLAUGHTER. SO, NO, WE'RE DOING GREAT ON ECONOMICS, GREAT ON TARIFFS. TARIFFS IS JUST A PART OF IT. WE'RE DRILLING LIKE NEVER BEFORE. THAT'S HOW THE OIL GOT DOWN LOW. MARIA: THE SUPREME COURT VICTORY THAT YOU GOT TODAY, FRIDAY, DO YOU THINK THAT COULD APPLY TODAY THE TARIFF LITIGATION THAT'S PENDING HERE IN TERMS OF THE LEGALITY OF YOUR TARIFFS? DOES THAT AFFECT THAT AT ALL IN. >> IT COULD. BUT, AGAIN, THE TARIFF LITIGATION, IF YOU LOOK AT IT, THERE ARE ALTERNATIVES IF WE HAVE TO USE THEM. THEY'RE NOT AS GOOD BECAUSE THEY'RE NOT AS DIRECT, THEY'RE NOT AS BITING, BUT THEY'RE VERY GOOD. SO YOU HAVE ALTERNATIVES. BUT WE WANT TO USE THIS PARTICULAR ASSET, THIS PARTICULAR GROUPING, THESE -- THE WORDS OF THIS ARE SO BEAUTIFUL. THEY WERE MEANT FOR IT. BUT WHAT HAPPENS WITH THE TARIFFS IS THAT IF A JUDGE RULED AGAINST US, AND THIS IS PART OF HOW YOU WIN, FAIRNESS. FAIRNESS IS ALSO A PART OF THE LAW. IF A JUDGE RULED AGAINST US ON TARIFFS, WE WOULD FALL PREY TO OTHER COUNTRIES DESTROYING US. WE CAN'T DO THAT. IF THEY DO TARIFFS ON US, WE DO TARIFFS ON THEM. IF THEY CHARGE US, WE CHARGE THEM. THEN THEY NEVER CHAR\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "transcript = fetch_transcript.run(video_id)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "transcript = fetch_transcript.run(video_id)\n",
    "print(f\"Retrieved transcript with {len(transcript)} characters\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Get video thumbnails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": [
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/mq3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/mq3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/mq2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/mq2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/mq1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/mq1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hq3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/hq3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hq2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/hq2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hq1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/hq1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/sd3.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/sd3.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/sd2.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/sd2.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/sd1.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/sd1.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/default.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/default.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/mqdefault.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/mqdefault.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/0.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/0.webp",
        "width": null
       },
       {
        "height": 94,
        "resolution": "168x94",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hqdefault.jpg?sqp=-oaymwEbCKgBEF5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLAeGC08pDrKd58uhqsL45MrITnh1w",
        "width": 168
       },
       {
        "height": 110,
        "resolution": "196x110",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hqdefault.jpg?sqp=-oaymwEbCMQBEG5IVfKriqkDDggBFQAAiEIYAXABwAEG&rs=AOn4CLCe54V_FIOXJwSjAFc3bW1TS3zeHg",
        "width": 196
       },
       {
        "height": 138,
        "resolution": "246x138",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hqdefault.jpg?sqp=-oaymwEcCPYBEIoBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLB8-2yjmyLwPF43jaxZPZk88C3ptw",
        "width": 246
       },
       {
        "height": 188,
        "resolution": "336x188",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hqdefault.jpg?sqp=-oaymwEcCNACELwBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLCOed_YcHkWjFDAm0Xj8Hf6o5bTvA",
        "width": 336
       },
       {
        "height": 360,
        "resolution": "480x360",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hqdefault.jpg",
        "width": 480
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/hqdefault.webp",
        "width": null
       },
       {
        "height": 480,
        "resolution": "640x480",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/sddefault.jpg",
        "width": 640
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/sddefault.webp",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/hq720.jpg",
        "width": null
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/hq720.webp",
        "width": null
       },
       {
        "height": 1080,
        "resolution": "1920x1080",
        "url": "https://i.ytimg.com/vi/YdqKZDJQoDw/maxresdefault.jpg",
        "width": 1920
       },
       {
        "height": null,
        "resolution": "",
        "url": "https://i.ytimg.com/vi_webp/YdqKZDJQoDw/maxresdefault.webp",
        "width": null
       }
      ],
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "thumbnails=get_thumbnails.run(\"https://www.youtube.com/watch?v=YdqKZDJQoDw&ab_channel=FoxNews\")\n",
    "\n",
    "display(JSON(thumbnails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video thumbnails\n",
    "thumbnails = get_thumbnails.run(youtube_url)\n",
    "print(f\"Retrieved {len(thumbnails)} thumbnails\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a comprehensive prompt to be passed to LLM to generate a summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "prompt = f\"\"\"\n",
    "Please analyze this YouTube video and provide a comprehensive summary.\n",
    "\n",
    "VIDEO TITLE: {meta_data['title']}\n",
    "CHANNEL: {meta_data['channel']}\n",
    "VIEWS: {meta_data['views']}\n",
    "DURATION: {meta_data['duration']} seconds\n",
    "LIKES: {meta_data['likes']}\n",
    "\n",
    "TRANSCRIPT EXCERPT:\n",
    "{transcript[:3000]}... (transcript truncated for brevity)\n",
    "\n",
    "Based on this information, please provide:\n",
    "1. A concise summary of the video content (3-5 bullet points)\n",
    "2. The main topics or themes discussed\n",
    "3. The intended audience for this content\n",
    "4. A brief analysis of why this video might be performing well (or not)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for prompt</summary>\n",
    "\n",
    "```python\n",
    "prompt = f\"\"\"\n",
    "Please analyze this YouTube video and provide a comprehensive summary.\n",
    "\n",
    "VIDEO TITLE: {video_metadata['title']}\n",
    "CHANNEL: {video_metadata['channel']}\n",
    "VIEWS: {video_metadata['views']}\n",
    "DURATION: {video_metadata['duration']} seconds\n",
    "LIKES: {video_metadata['likes']}\n",
    "\n",
    "TRANSCRIPT EXCERPT:\n",
    "{transcript[:3000]}... (transcript truncated for brevity)\n",
    "\n",
    "Based on this information, please provide:\n",
    "1. A concise summary of the video content (3-5 bullet points)\n",
    "2. The main topics or themes discussed\n",
    "3. The intended audience for this content\n",
    "4. A brief analysis of why this video might be performing well (or not)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Single LLM invocation with all the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "messages = [HumanMessage(content=prompt)]\n",
    "response = llm.invoke(messages)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Display the comprehensive analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Summary of the Video Content\n",
      "\n",
      "1. Former President Donald Trump reflects on recent significant events, including US military strikes against Iran and his diplomatic efforts with Middle Eastern leaders.\n",
      "2. Trump discusses the positive economic indicators and growth during his presidency, attributing the turnaround to his administration's policies.\n",
      "3. He highlights negotiations with Iran regarding their nuclear program, indicating a tough stance against allowing Iran to pursue nuclear weaponry.\n",
      "4. Trump recalls the relationships built with Saudi Arabia, Qatar, and the UAE, emphasizing their support and investment in the US.\n",
      "5. He shares his sentiments on Iran's enrichment program, labeling it a negative aspect of their nuclear ambitions and recounts the military actions taken in response.\n",
      "\n",
      "### Main Topics or Themes Discussed\n",
      "\n",
      "- **US-Iran Relations:** Discussion about the military strikes against Iran and negotiations regarding nuclear capabilities.\n",
      "- **Economic Performance:** Trump's view of the economic recovery in the US during his presidency, including inflation and investment figures.\n",
      "- **Diplomatic Engagement:** Emphasis on strengthening ties with Middle Eastern countries and foreign investments.\n",
      "- **Military Action and Strategy:** Insights into the planning and execution of military operations against perceived threats.\n",
      "\n",
      "### Intended Audience for This Content\n",
      "\n",
      "The intended audience appears to be:\n",
      "\n",
      "- Supporters of Donald Trump and conservative viewers likely aligned with Fox News' demographic.\n",
      "- Individuals interested in US foreign policy, particularly regarding the Middle East.\n",
      "- Those looking for updates on Trump's presidency and his perspective on key geopolitical issues.\n",
      "\n",
      "### Analysis of Video Performance\n",
      "\n",
      "This video may be performing well for several reasons:\n",
      "\n",
      "- **High Viewership:** With over 1 million views, the content resonates strongly within a politically engaged audience, particularly followers of Trump.\n",
      "- **Controversial Topics:** Discussions on military action against Iran and nuclear negotiations are inherently provocative and can draw significant interest.\n",
      "- **Familiar Format:** The interview style with a recognizable host (Maria Bartiromo) may appeal to regular Fox News viewers, creating a sense of familiarity and trust.\n",
      "- **Engagement with Current Events:** The video taps into ongoing debates about US foreign policy and economic performance, making it relevant to contemporary discussions.\n",
      "- **Emotional Appeals:** Trump's use of personal anecdotes and confident declarations might invoke strong reactions, whether support or opposition. This emotional engagement can elicit active viewership and sharing. \n",
      "\n",
      "Overall, the combination of Trump's controversial persona, significant geopolitical themes, and positive economic messaging likely contributes to the video's successful reception.\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for hint</summary>\n",
    "\n",
    "```python\n",
    "# Get video transcript\n",
    "print(\"\\n===== VIDEO ANALYSIS =====\\n\")\n",
    "print(response.content)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kunal Makwana](https://author.skills.network/instructors/kunal_makwana) is a Data Scientist at IBM and is currently pursuing his Master's in Computer Science at Dalhousie University.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://author.skills.network/instructors/joseph_santarcangelo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "535e148d01d12998c9766a3ee8486df2976a4646226066d61e82f992ec66f89a"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
